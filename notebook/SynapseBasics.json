{
	"name": "SynapseBasics",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "62eca53e-935e-4a8a-a132-7966e491ab6f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%md\r\n",
					"# Reading and Writing to Synapse\r\n",
					" \r\n",
					" ## Objectives\r\n",
					"\r\n",
					" * Describe the connection architecture of Synapse and Spark\r\n",
					" * Configure a connection between Databricks and Synapse\r\n",
					" * Read data from Synapse\r\n",
					" * Write data to Synapse\r\n",
					" \r\n",
					" ### Azure Synapse\r\n",
					" - leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data\r\n",
					" - PolyBase T-SQL queries\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" ## Synapse Connector\r\n",
					" - uses Azure Blob Storage as intermediary\r\n",
					" - uses PolyBase in Synapse\r\n",
					" - enables MPP reads and writes to Synapse from Azure Databricks\r\n",
					" \r\n",
					" Note: The Synapse connector is more suited to ETL than to interactive queries. For interactive and ad-hoc queries, data should be extracted into a Databricks Delta table.\r\n",
					" \r\n",
					" ```\r\n",
					"                            ┌─────────┐\r\n",
					"       ┌───────────────────>│ STORAGE │<──────────────────┐\r\n",
					"       │ Storage acc key /  │ ACCOUNT │ Storage acc key / │\r\n",
					"       │ Managed Service ID └─────────┘ OAuth 2.0         │\r\n",
					"       │                         │                        │\r\n",
					"       │                         │ Storage acc key /      │\r\n",
					"       │                         │ OAuth 2.0              │\r\n",
					"       v                         v                 ┌──────v────┐\r\n",
					" ┌──────────┐              ┌──────────┐            │┌──────────┴┐\r\n",
					" │ Synapse  │              │  Spark   │            ││ Spark     │\r\n",
					" │ Analytics│<────────────>│  Driver  │<───────────>| Executors │\r\n",
					" └──────────┘  JDBC with   └──────────┘ Configured  └───────────┘\r\n",
					"               username &               in Spark\r\n",
					"               password\r\n",
					" ```\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" \r\n",
					" ## SQL DW Connection\r\n",
					" \r\n",
					" Three connections are made to exchange queries and data between Databricks and Synapse\r\n",
					" 1. **Spark driver to Synapse**\r\n",
					"    - the Spark driver connects to Synapse via JDBC using a username and password\r\n",
					" 2. **Spark driver and executors to Azure Blob Storage**\r\n",
					"    - the Azure Blob Storage container acts as an intermediary to store bulk data when reading from or writing to Synapse\r\n",
					"    - Spark connects to the Blob Storage container using the Azure Blob Storage connector bundled in Databricks Runtime\r\n",
					"    - the URI scheme for specifying this connection must be wasbs\r\n",
					"    - the credential used for setting up this connection must be a storage account access key\r\n",
					"    - the account access key is set in the session configuration associated with the notebook that runs the command\r\n",
					"    - this configuration does not affect other notebooks attached to the same cluster. `spark` is the SparkSession object provided in the notebook\r\n",
					" 3. **Synapse to Azure Blob Storage**\r\n",
					"    - Synapse also connects to the Blob Storage container during loading and unloading of temporary data\r\n",
					"    - set `forwardSparkAzureStorageCredentials` to true\r\n",
					"    - the forwarded storage access key is represented by a temporary database scoped credential in the Synapse instance\r\n",
					"    - Synapse connector creates a database scoped credential before asking Synapse to load or unload data\r\n",
					"    - then it deletes the database scoped credential once the loading or unloading operation is done.\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" ## Enter Variables from Cloud Setup\r\n",
					" \r\n",
					" Before starting this lesson, you were guided through configuring Azure Synapse and deploying a Storage Account and blob container.\r\n",
					" \r\n",
					" In the cell below, enter the **Storage Account Name**, the **Container Name**, and the **Access Key** for the blob container you created.\r\n",
					" \r\n",
					" Also enter the JDBC connection string for your Azure Synapse instance. Make sure you substitute in your password as indicated within the generated string.\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					"storageAccount = \"name-of-your-storage-account\"\r\n",
					"containerName = \"data\"\r\n",
					"accessKey = \"your-storage-key\"\r\n",
					"jdbcURI = \"\"\r\n",
					"\r\n",
					"spark.conf.set(f\"fs.azure.account.key.{storageAccount}.blob.core.windows.net\", accessKey)\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" ## Read from the Customer Table\r\n",
					" \r\n",
					" Next, use the Synapse Connector to read data from the Customer Table.\r\n",
					" \r\n",
					" Use the read to define a tempory table that can be queried.\r\n",
					" \r\n",
					" Note:\r\n",
					" \r\n",
					" - the connector uses a caching directory on the Azure Blob Container.\r\n",
					" - `forwardSparkAzureStorageCredentials` is set to `true` so that the Synapse instance can access the blob for its MPP read via Polybase\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					"cacheDir = f\"wasbs://{containerName}@{storageAccount}.blob.core.windows.net/cacheDir\"\r\n",
					"\r\n",
					"tableName = \"dbo.DimCustomer\"\r\n",
					"\r\n",
					"customerDF = (spark.read\r\n",
					"  .format(\"com.databricks.spark.sqldw\")\r\n",
					"  .option(\"url\", jdbcURI)\r\n",
					"  .option(\"tempDir\", cacheDir)\r\n",
					"  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\r\n",
					"  .option(\"dbTable\", tableName)\r\n",
					"  .load())\r\n",
					"\r\n",
					"customerDF.createOrReplaceTempView(\"customer_data\")\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" \r\n",
					" Use SQL queries to count the number of rows in the Customer table and to display table metadata.\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %sql\r\n",
					" select count(*) from customer_data\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %sql\r\n",
					" describe customer_data\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" \r\n",
					" Note that `CustomerKey` and `CustomerAlternateKey` use a very similar naming convention.\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %sql\r\n",
					" select CustomerKey, CustomerAlternateKey from customer_data limit 10;\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" \r\n",
					" In a situation in which we may be merging many new customers into this table, we can imagine that we may have issues with uniqueness with regard to the `CustomerKey`. Let us redefine `CustomerAlternateKey` for stronger uniqueness using a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).\r\n",
					" \r\n",
					" To do this we will define a UDF and use it to transform the `CustomerAlternateKey` column. Once this is done, we will write the updated Customer Table to a Staging table.\r\n",
					" \r\n",
					" **Note:** It is a best practice to update the Synapse instance via a staging table.\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					"import uuid\r\n",
					"\r\n",
					"from pyspark.sql.types import StringType\r\n",
					"from pyspark.sql.functions import udf\r\n",
					"\r\n",
					"uuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\r\n",
					"customerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\r\n",
					"display(customerUpdatedDF)\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" \r\n",
					" ### Use the Polybase Connector to Write to the Staging Table\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					"(customerUpdatedDF.write\r\n",
					"  .format(\"com.databricks.spark.sqldw\")\r\n",
					"  .mode(\"overwrite\")\r\n",
					"  .option(\"url\", jdbcURI)\r\n",
					"  .option(\"forward_spark_azure_storage_credentials\", \"true\")\r\n",
					"  .option(\"dbtable\", tableName + \"Staging\")\r\n",
					"  .option(\"tempdir\", cacheDir)\r\n",
					"  .save())\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %md\r\n",
					" ## Read and Display Changes from Staging Table\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					"customerTempDF = (spark.read\r\n",
					"  .format(\"com.databricks.spark.sqldw\")\r\n",
					"  .option(\"url\", jdbcURI)\r\n",
					"  .option(\"tempDir\", cacheDir)\r\n",
					"  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\r\n",
					"  .option(\"dbTable\", tableName + \"Staging\")\r\n",
					"  .load())\r\n",
					"\r\n",
					"customerTempDF.createOrReplaceTempView(\"customer_temp_data\")\r\n",
					"\r\n",
					"# COMMAND ----------\r\n",
					"\r\n",
					" %sql\r\n",
					" select CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;"
				],
				"execution_count": null
			}
		]
	}
}