{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Work with Notebooks\r\n",
        "\r\n",
        "**Technical Accomplishments:**\r\n",
        "- Set the stage for learning on the Databricks platform\r\n",
        "- Demonstrate how to develop & execute code within a notebook\r\n",
        "- Introduce the Databricks File System (DBFS)\r\n",
        "- Introduce `dbutils`\r\n",
        "- Review the various \"Magic Commands\"\r\n",
        "- Review various built-in commands that facilitate working with the notebooks\r\n",
        "\r\n",
        "### Feeling Lost?\r\n",
        "The [Databricks Unified Support Portal](https://help.databricks.com/s/) is a great place to search forums and documentation for Databricks and Spark.\r\n",
        "\r\n",
        "Databricks also offers [multiple tiers for dedicated support](https://databricks.com/support)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/wiki-book/general/logo_spark_tiny.png) Magic Commands\r\n",
        "* Magic Commands are specific to the Databricks notebooks\r\n",
        "* They are very similar to Magic Commands found in comparable notebook products\r\n",
        "* These are built-in commands that do not apply to the notebook's default language\r\n",
        "* A single percent (%) symbol at the start of a cell identifies a Magic Commands\r\n",
        "\r\n",
        "### Magic Command: &percnt;sh\r\n",
        "For example, **&percnt;sh** allows us to execute shell commands on the driver\r\n",
        "Additional Magic Commands allow for the execution of code in languages other than the notebook's default:\r\n",
        "* **&percnt;python**\r\n",
        "* **&percnt;scala**\r\n",
        "* **&percnt;sql**\r\n",
        "* **&percnt;r**\r\n",
        "\r\n",
        "Links/Embedded HTML: <a href=\"http://bfy.tw/19zq\" target=\"_blank\">What is Markdown?</a>\r\n",
        "\r\n",
        "### Magic Command: &percnt;run\r\n",
        "* You can run a notebook from another notebook by using the Magic Command **%run**\r\n",
        "* All variables & functions defined in that other notebook will become available in your current notebook\r\n",
        "\r\n",
        "For example, The following cell should fail to execute because the variable `username` has not yet been declared:\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/wiki-book/general/logo_spark_tiny.png) Databricks File System - DBFS\r\n",
        "* DBFS is a layer over a cloud-based object store\r\n",
        "* Files in DBFS are persisted to the object store\r\n",
        "* The lifetime of files in the DBFS are **NOT** tied to the lifetime of our cluster\r\n",
        "\r\n",
        "### Mounting Data into DBFS\r\n",
        "* Mounting other object stores into DBFS gives Databricks users access via the file system\r\n",
        "* This is just one of many techniques for pulling data into Spark\r\n",
        "* The datasets needed for this class have already been mounted for us with the call to `%run \"../Includes/Classroom Setup\"`\r\n",
        "* We will confirm that in just a few minutes\r\n",
        "\r\n",
        "See also <a href=\"https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html\" target=\"_blank\">Databricks File System - DBFS</a>.\r\n",
        "\r\n",
        "## Databricks Utilities - dbutils\r\n",
        "* You can access the DBFS through the Databricks Utilities class (and other file IO routines).\r\n",
        "* An instance of DBUtils is already declared for us as `dbutils`.\r\n",
        "* For in-notebook documentation on DBUtils you can execute the command `dbutils.help()`.\r\n",
        "\r\n",
        "Additional help is available for each sub-utility:\r\n",
        "* `dbutils.fs.help()`\r\n",
        "* `dbutils.meta.help()`\r\n",
        "* `dbutils.notebook.help()`\r\n",
        "* `dbutils.widgets.help()`\r\n",
        "\r\n",
        "Let's take a look at the file system utilities, `dbutils.fs`\r\n",
        "\r\n",
        "### dbutils.fs.mounts()\r\n",
        "* As previously mentioned, all our datasets should already be mounted\r\n",
        "* We can use `dbutils.fs.mounts()` to verify that assertion\r\n",
        "* This method returns a collection of `MountInfo` objects, one for each mount\r\n",
        "\r\n",
        "### dbutils.fs.ls(..)\r\n",
        "* And now we can use `dbutils.fs.ls(..)` to view the contents of that mount\r\n",
        "* This method returns a collection of `FileInfo` objects, one for each item in the specified directory\r\n",
        "\r\n",
        "See also <a href=\"https://docs.azuredatabricks.net/api/latest/dbfs.html#dbfsfileinfo\" target=\"_blank\">FileInfo</a>\r\n",
        "\r\n",
        "### display(..)\r\n",
        "\r\n",
        "Besides printing each item returned from `dbutils.fs.ls(..)` we can also pass that collection to another Databricks specific command called `display(..)`.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = dbutils.fs.ls(\"/mnt/training/\")\r\n",
        "\r\n",
        "for fileInfo in files:\r\n",
        "  print(fileInfo.path)\r\n",
        "\r\n",
        "print(\"-\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `display(..)` command is overloaded with a lot of other capabilities:\r\n",
        "* Presents up to 1000 records.\r\n",
        "* Exporting data as CSV.\r\n",
        "* Rendering a multitude of different graphs.\r\n",
        "* Rendering geo-located data on a world map.\r\n",
        "\r\n",
        "And as we will see later, it is also an excellent tool for previewing our data in a notebook."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Magic Command: &percnt;fs\r\n",
        "\r\n",
        "There is at least one more trick for looking at the DBFS.\r\n",
        "\r\n",
        "It is a wrapper around `dbutils.fs` and it is the Magic Command known as **&percnt;fs**.\r\n",
        "\r\n",
        "The following call is equivalent to the previous call, `display( dbutils.fs.ls(\"/mnt/training\") )` - there is no real difference between the two."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/wiki-book/general/logo_spark_tiny.png) Learning More\r\n",
        "\r\n",
        "We like to encourage you to explore the documentation to learn more about the various features of the Databricks platform and notebooks.\r\n",
        "* <a href=\"https://docs.azuredatabricks.net/user-guide/index.html\" target=\"_blank\">User Guide</a>\r\n",
        "* <a href=\"https://docs.databricks.com/user-guide/getting-started.html\" target=\"_blank\">Getting Started with Databricks</a>\r\n",
        "* <a href=\"https://docs.azuredatabricks.net/user-guide/notebooks/index.html\" target=\"_blank\">User Guide / Notebooks</a>\r\n",
        "* <a href=\"https://docs.databricks.com/user-guide/notebooks/index.html#importing-notebooks\" target=\"_blank\">Importing notebooks - Supported Formats</a>\r\n",
        "* <a href=\"https://docs.azuredatabricks.net/administration-guide/index.html\" target=\"_blank\">Administration Guide</a>\r\n",
        "* <a href=\"https://docs.databricks.com/user-guide/clusters/index.html\" target=\"_blank\">Cluster Configuration</a>\r\n",
        "* <a href=\"https://docs.azuredatabricks.net/api/index.html\" target=\"_blank\">REST API</a>\r\n",
        "* <a href=\"https://docs.azuredatabricks.net/release-notes/index.html\" target=\"_blank\">Release Notes</a>\r\n",
        "* <a href=\"https://docs.azuredatabricks.net\" target=\"_blank\">And much more!</a>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It's worth noting that in Spark 2.0 `SparkSession` is a replacement for the other entry points:\r\n",
        "* `SparkContext`, available in our notebook as **sc**.\r\n",
        "* `SQLContext`, or more specifically it's subclass `HiveContext`, available in our notebook as **sqlContext**.\r\n",
        "\r\n",
        "Before we can dig into the functionality of the `SparkSession` class, we need to know how to access the API documentation for Apache Spark.\r\n",
        "\r\n",
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Spark API\r\n",
        "\r\n",
        "### **Spark API Home Page**\r\n",
        " 0. Open a new browser tab.\r\n",
        " 0. Search for **Spark API Latest** or **Spark API _x.x.x_** for a specific version.\r\n",
        " 0. Select **Spark API Documentation - Spark _x.x.x_ Documentation - Apache Spark**. \r\n",
        " 0. Which set of documentation you will use depends on which language you will use.\r\n",
        " \r\n",
        " Other Documentation:\r\n",
        " * Programming Guides for DataFrames, SQL, Graphs, Machine Learning, Streaming...\r\n",
        " * Deployment Guides for Spark Standalone, Mesos, Yarn...\r\n",
        " * Configuration, Monitoring, Tuning, Security...\r\n",
        " \r\n",
        " Here are some shortcuts\r\n",
        "   * <a href=\"https://spark.apache.org/docs/latest/api/\" target=\"_blank\">Spark API Documentation - Latest</a>\r\n",
        "   * <a href=\"https://spark.apache.org/docs/2.4.0/\" target=\"_blank\">Spark API Documentation - 2.4.0</a>\r\n",
        "   * <a href=\"https://spark.apache.org/docs/2.2.0/\" target=\"_blank\">Spark API Documentation - 2.2.0</a>\r\n",
        "   * <a href=\"https://spark.apache.org/docs/2.1.1/\" target=\"_blank\">Spark API Documentation - 2.1.1</a>\r\n",
        "   * <a href=\"https://spark.apache.org/docs/2.0.2/\" target=\"_blank\">Spark API Documentation - 2.0.2</a>\r\n",
        "   * <a href=\"https://spark.apache.org/docs/1.6.3/\" target=\"_blank\">Spark API Documentation - 1.6.3</a>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark API (Scala)\r\n",
        " \r\n",
        " 0. Select **Spark Scala API (Scaladoc)**.\r\n",
        " 0. Look up the documentation for `org.apache.spark.sql.SparkSession`.\r\n",
        "   0. In the upper-left-hand-corner type **SparkSession** into the search field.\r\n",
        "   0. The search will execute automatically.\r\n",
        "   0. In the class/package list, click on **SparkSession**.\r\n",
        "   0. The documentation should open in the right-hand pane.\r\n",
        "\r\n",
        "\r\n",
        " ### Spark API (Python)\r\n",
        " \r\n",
        " 0. Select **Spark Python API (Sphinx)**.\r\n",
        " 0. Look up the documentation for `pyspark.sql.SparkSession`.\r\n",
        "   0. In the lower-left-hand-corner type **SparkSession** into the search field.\r\n",
        "   0. Hit **[Enter]**.\r\n",
        "   0. The search results should appear in the right-hand pane.\r\n",
        "   0. Click on **pyspark.sql.SparkSession (Python class, in pyspark.sql module)**\r\n",
        "   0. The documentation should open in the right-hand pane."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) SparkSession\r\n",
        " \r\n",
        " Quick function review:\r\n",
        " * `createDataSet(..)`\r\n",
        " * `createDataFrame(..)`\r\n",
        " * `emptyDataSet(..)`\r\n",
        " * `emptyDataFrame(..)`\r\n",
        " * `range(..)`\r\n",
        " * `read(..)`\r\n",
        " * `readStream(..)`\r\n",
        " * `sparkContext(..)`\r\n",
        " * `sqlContext(..)`\r\n",
        " * `sql(..)`\r\n",
        " * `streams(..)`\r\n",
        " * `table(..)`\r\n",
        " * `udf(..)`\r\n",
        " \r\n",
        " The function we are most interested in is `SparkSession.read()` which returns a `DataFrameReader`.\r\n",
        "\r\n",
        " ##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) DataFrameReader\r\n",
        " \r\n",
        " Look up the documentation for `DataFrameReader`.\r\n",
        " \r\n",
        " Quick function review:\r\n",
        " * `csv(path)`\r\n",
        " * `jdbc(url, table, ..., connectionProperties)`\r\n",
        " * `json(path)`\r\n",
        " * `format(source)`\r\n",
        " * `load(path)`\r\n",
        " * `orc(path)`\r\n",
        " * `parquet(path)`\r\n",
        " * `table(tableName)`\r\n",
        " * `text(path)`\r\n",
        " * `textFile(path)`\r\n",
        " \r\n",
        " Configuration methods:\r\n",
        " * `option(key, value)`\r\n",
        " * `options(map)`\r\n",
        " * `schema(schema)`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from CSV w/InferSchema\r\n",
        " \r\n",
        " We are going to start by reading in a very simple text file.\r\n",
        "\r\n",
        "  ### The Data Source\r\n",
        " * For this exercise, we will be using a tab-separated file called **pageviews_by_second.tsv** (255 MB file from Wikipedia)\r\n",
        " * We can use **&percnt;fs ls ...** to view the file on the DBFS.\r\n",
        "\r\n",
        " We can use **&percnt;fs head ...** to peek at the first couple thousand characters of the file.\r\n",
        "\r\n",
        " %fs head /mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\r\n",
        "\r\n",
        "  There are a couple of things to note here:\r\n",
        " * The file has a header.\r\n",
        " * The file is tab separated (we can infer that from the file extension and the lack of other characters between each \"column\").\r\n",
        " * The first two columns are strings and the third is a number.\r\n",
        " \r\n",
        " Knowing those details, we can read in the \"CSV\" file."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step #1 - Read The CSV File\r\n",
        " Let's start with the bare minimum by specifying the tab character as the delimiter and the location of the file:\r\n",
        "\r\n",
        " "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A reference to our tab-separated-file\r\n",
        "csvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\r\n",
        "\r\n",
        "tempDF = (spark.read           # The DataFrameReader\r\n",
        "   .option(\"sep\", \"\\t\")        # Use tab delimiter (default is comma-separator)\r\n",
        "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is guaranteed to <u>trigger one job</u>.\r\n",
        " \r\n",
        " A *Job* is triggered anytime we are \"physically\" __required to touch the data__.\r\n",
        " \r\n",
        " In some cases, __one action may create multiple jobs__ (multiple reasons to touch the data).\r\n",
        " \r\n",
        " In this case, the reader has to __\"peek\" at the first line__ of the file to determine how many columns of data we have.\r\n",
        "\r\n",
        "We can see the structure of the `DataFrame` by executing the command `printSchema()`\r\n",
        " \r\n",
        " It prints to the console the name of each column, its data type and if it's null or not.\r\n",
        " \r\n",
        " ** *Note:* ** *We will be covering the other `DataFrame` functions in other notebooks.* \r\n",
        "\r\n",
        "We can see from the schema that...\r\n",
        " * there are three columns\r\n",
        " * the column names **_c0**, **_c1**, and **_c2** (automatically generated names)\r\n",
        " * all three columns are **strings**\r\n",
        " * all three columns are **nullable**\r\n",
        " \r\n",
        " And if we take a quick peek at the data, we can see that line #1 contains the headers and not data: "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempDF.printSchema()\r\n",
        "\r\n",
        "display(tempDF)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step #2 - Use the File's Header\r\n",
        " Next, we can add an option that tells the reader that the data contains a header and to use that header to determine our column names.\r\n",
        " \r\n",
        " ** *NOTE:* ** *We know we have a header based on what we can see in \"head\" of the file from earlier.*\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(spark.read                    # The DataFrameReader\r\n",
        "   .option(\"sep\", \"\\t\")        # Use tab delimiter (default is comma-separator)\r\n",
        "   .option(\"header\", \"true\")   # Use first line of all files as header\r\n",
        "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\r\n",
        "   .printSchema()\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A couple of notes about this iteration:\r\n",
        " * again, only one job\r\n",
        " * there are three columns\r\n",
        " * all three columns are **strings**\r\n",
        " * all three columns are **nullable**\r\n",
        " * the column names are specified: **timestamp**, **site**, and **requests** (the change we were looking for)\r\n",
        " \r\n",
        " A \"peek\" at the first line of the file is all that the reader needs to determine the number of columns and the name of each column.\r\n",
        " \r\n",
        " Before going on, make a note of the duration of the previous call - it should be just under 3 seconds."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step #3 - Infer the Schema\r\n",
        " \r\n",
        " Lastly, we can add an option that tells the reader to infer each column's data type (aka the schema)\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(spark.read                        # The DataFrameReader\r\n",
        "   .option(\"header\", \"true\")       # Use first line of all files as header\r\n",
        "   .option(\"sep\", \"\\t\")            # Use tab delimiter (default is comma-separator)\r\n",
        "   .option(\"inferSchema\", \"true\")  # Automatically infer data types\r\n",
        "   .csv(csvFile)                   # Creates a DataFrame from CSV after reading in the file\r\n",
        "   .printSchema()\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Review: Reading CSV w/InferSchema\r\n",
        " * we still have three columns\r\n",
        " * all three columns are still **nullable**\r\n",
        " * all three columns have their proper names\r\n",
        " * two jobs were executed (not one as in the previous example)\r\n",
        " * our three columns now have distinct data types:\r\n",
        "   * **timestamp** == **timestamp**\r\n",
        "   * **site** == **string**\r\n",
        "   * **requests** == **integer**\r\n",
        " \r\n",
        " **Question:** Why were there two jobs?\r\n",
        " \r\n",
        " **Question:** How long did the last job take?\r\n",
        " \r\n",
        " **Question:** Why did it take so much longer?\r\n",
        " \r\n",
        " Discuss..."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a list of all the options related to reading CSV files, please see the documentation for `DataFrameReader.csv(..)`\r\n",
        "\r\n",
        "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csvDF = (spark.read\r\n",
        "  .option('header', 'true')\r\n",
        "  .option('sep', \"\\t\")\r\n",
        "  .schema(csvSchema)\r\n",
        "  .csv(csvFile)\r\n",
        ")\r\n",
        "print(\"Partitions: \" + str(csvDF.rdd.getNumPartitions()) )\r\n",
        "printRecordsPerPartition(csvDF)\r\n",
        "print(\"-\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data - JSON Files\r\n",
        "\r\n",
        "Much like the CSV reader, the JSON reader also assumes...\r\n",
        "* That there is one JSON object per line and...\r\n",
        "* That it's delineated by a new-line.\r\n",
        "\r\n",
        "This format is referred to as **JSON Lines** or **newline-delimited JSON** \r\n",
        "\r\n",
        "More information about this format can be found at <a href=\"http://jsonlines.org/\" target=\"_blank\">http://jsonlines.org</a>.\r\n",
        "\r\n",
        "** *Note:* ** *Spark 2.2 was released on July 11th 2016. With that comes File IO improvements for CSV & JSON, but more importantly, **Support for parsing multi-line JSON and CSV files**. You can read more about that (and other features in Spark 2.2) in the <a href=\"https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html\" target=\"_blank\">Databricks Blog</a>.*\r\n",
        "\r\n",
        "### The Data Source\r\n",
        "* For this exercise, we will be using the file called **snapshot-2016-05-26.json** (<a href=\"https://wikitech.wikimedia.org/wiki/Stream.wikimedia.org/rc\" target=\"_blank\">4 MB</a> file from Wikipedia).\r\n",
        "* The data represents a set of edits to Wikipedia articles captured in May of 2016.\r\n",
        "* It's located on the DBFS at **dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json**\r\n",
        "* Like we did with the CSV file, we can use **&percnt;fs ls ...** to view the file on the DBFS.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read The JSON File\r\n",
        "\r\n",
        "The command to read in JSON looks very similar to that of CSV.\r\n",
        "\r\n",
        "In addition to reading the JSON file, we will also print the resulting schema.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsonFile = \"dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json\"\r\n",
        "\r\n",
        "wikiEditsDF = (spark.read           # The DataFrameReader\r\n",
        "    .option(\"inferSchema\", \"true\")  # Automatically infer data types & column names\r\n",
        "    .json(jsonFile)                 # Creates a DataFrame from JSON after reading in the file\r\n",
        " )\r\n",
        "wikiEditsDF.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our DataFrame created, we can now take a peak at the data.\r\n",
        "\r\n",
        "But to demonstrate a unique aspect of JSON data (or any data with embedded fields), we will first create a temporary view and then view the data via SQL:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a view called wiki_edits\r\n",
        "wikiEditsDF.createOrReplaceTempView(\"wiki_edits\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can take a peak at the data with simple SQL SELECT statement:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "\r\n",
        "SELECT * FROM wiki_edits "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the **geocoding** column has embedded data.\r\n",
        "\r\n",
        "You can expand the fields by clicking the right triangle in each row.\r\n",
        "\r\n",
        "But we can also reference the sub-fields directly as we see in the following SQL statement:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "\r\n",
        "SELECT channel, page, geocoding.city, geocoding.latitude, geocoding.longitude \r\n",
        "FROM wiki_edits \r\n",
        "WHERE geocoding.city IS NOT NULL"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review: Reading from JSON w/ InferSchema\r\n",
        "\r\n",
        "While there are similarities between reading in CSV & JSON there are some key differences:\r\n",
        "* We only need one job even when inferring the schema.\r\n",
        "* There is no header which is why there isn't a second job in this case - the column names are extracted from the JSON object's attributes.\r\n",
        "* Unlike CSV which reads in 100% of the data, the JSON reader only samples the data.  \r\n",
        "**Note:** In Spark 2.2 the behavior was changed to read in the entire JSON file.\r\n",
        "\r\n",
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ User-Defined Schema\r\n",
        "\r\n",
        "To avoid the extra job, we can (just like we did with CSV) specify the schema for the `DataFrame`.\r\n",
        "\r\n",
        "### Step #1 - Create the Schema\r\n",
        "\r\n",
        "Compared to our CSV example, the structure of this data is a little more complex.\r\n",
        "\r\n",
        "Note that we can support complex data types as seen in the field `geocoding`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required for StructField, StringType, IntegerType, etc.\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "jsonSchema = StructType([\r\n",
        "  StructField(\"channel\", StringType(), True),\r\n",
        "  StructField(\"comment\", StringType(), True),\r\n",
        "  StructField(\"delta\", IntegerType(), True),\r\n",
        "  StructField(\"flag\", StringType(), True),\r\n",
        "  StructField(\"geocoding\", StructType([\r\n",
        "    StructField(\"city\", StringType(), True),\r\n",
        "    StructField(\"country\", StringType(), True),\r\n",
        "    StructField(\"countryCode2\", StringType(), True),\r\n",
        "    StructField(\"countryCode3\", StringType(), True),\r\n",
        "    StructField(\"stateProvince\", StringType(), True),\r\n",
        "    StructField(\"latitude\", DoubleType(), True),\r\n",
        "    StructField(\"longitude\", DoubleType(), True)\r\n",
        "  ]), True),\r\n",
        "  StructField(\"isAnonymous\", BooleanType(), True),\r\n",
        "  StructField(\"isNewPage\", BooleanType(), True),\r\n",
        "  StructField(\"isRobot\", BooleanType(), True),\r\n",
        "  StructField(\"isUnpatrolled\", BooleanType(), True),\r\n",
        "  StructField(\"namespace\", StringType(), True),\r\n",
        "  StructField(\"page\", StringType(), True),\r\n",
        "  StructField(\"pageURL\", StringType(), True),\r\n",
        "  StructField(\"timestamp\", StringType(), True),\r\n",
        "  StructField(\"url\", StringType(), True),\r\n",
        "  StructField(\"user\", StringType(), True),\r\n",
        "  StructField(\"userURL\", StringType(), True),\r\n",
        "  StructField(\"wikipediaURL\", StringType(), True),\r\n",
        "  StructField(\"wikipedia\", StringType(), True)\r\n",
        "])\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was a lot of typing to get our schema!\r\n",
        "\r\n",
        "For a small file, manually creating the the schema may not be worth the effort.\r\n",
        "\r\n",
        "However, for a large file, the time to manually create the schema may be worth the trade off of a really long infer-schema process.\r\n",
        "\r\n",
        "### Step #2 - Read in the JSON\r\n",
        "\r\n",
        "Next, we will read in the JSON file and once again print its schema.\r\n",
        "\r\n",
        "### Review: Reading from JSON w/ User-Defined Schema\r\n",
        "* Just like CSV, providing the schema avoids the extra jobs.\r\n",
        "* The schema allows us to rename columns and specify alternate data types.\r\n",
        "* Can get arbitrarily complex in its structure."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "(spark.read            # The DataFrameReader\r\n",
        "  .schema(jsonSchema)  # Use the specified schema\r\n",
        "  .json(jsonFile)      # Creates a DataFrame from JSON after reading in the file\r\n",
        "  .printSchema()\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jsonDF = (spark.read\r\n",
        "  .schema(jsonSchema)\r\n",
        "  .json(jsonFile)    \r\n",
        ")\r\n",
        "print(\"Partitions: \" + str(jsonDF.rdd.getNumPartitions()))\r\n",
        "printRecordsPerPartition(jsonDF)\r\n",
        "print(\"-\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And of course we can view that data here:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(jsonDF)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from Parquet Files\r\n",
        "\r\n",
        "<strong style=\"font-size:larger\">\"</strong>Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.<strong style=\"font-size:larger\">\"</strong><br>\r\n",
        "\r\n",
        "### About Parquet Files\r\n",
        "* Free & Open Source.\r\n",
        "* Increased query performance over row-based data stores.\r\n",
        "* Provides efficient data compression.\r\n",
        "* Designed for performance on large data sets.\r\n",
        "* Supports limited schema evolution.\r\n",
        "* Is a splittable \"file format\".\r\n",
        "* A <a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Column-Oriented</a> data store\r\n",
        "\r\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;** Row Format ** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Column Format**\r\n",
        "\r\n",
        "<table style=\"border:0\">\r\n",
        "\r\n",
        "  <tr>\r\n",
        "    <th>ID</th><th>Name</th><th>Score</th>\r\n",
        "    <th style=\"border-top:0;border-bottom:0\">&nbsp;</th>\r\n",
        "    <th>ID:</th><td>1</td><td>2</td>\r\n",
        "    <td style=\"border-right: 1px solid #DDDDDD\">3</td>\r\n",
        "  </tr>\r\n",
        "\r\n",
        "  <tr>\r\n",
        "    <td>1</td><td>john</td><td>4.1</td>\r\n",
        "    <td style=\"border-top:0;border-bottom:0\">&nbsp;</td>\r\n",
        "    <th>Name:</th><td>john</td><td>mike</td>\r\n",
        "    <td style=\"border-right: 1px solid #DDDDDD\">sally</td>\r\n",
        "  </tr>\r\n",
        "\r\n",
        "  <tr>\r\n",
        "    <td>2</td><td>mike</td><td>3.5</td>\r\n",
        "    <td style=\"border-top:0;border-bottom:0\">&nbsp;</td>\r\n",
        "    <th style=\"border-bottom: 1px solid #DDDDDD\">Score:</th>\r\n",
        "    <td style=\"border-bottom: 1px solid #DDDDDD\">4.1</td>\r\n",
        "    <td style=\"border-bottom: 1px solid #DDDDDD\">3.5</td>\r\n",
        "    <td style=\"border-bottom: 1px solid #DDDDDD; border-right: 1px solid #DDDDDD\">6.4</td>\r\n",
        "  </tr>\r\n",
        "\r\n",
        "  <tr>\r\n",
        "    <td style=\"border-bottom: 1px solid #DDDDDD\">3</td>\r\n",
        "    <td style=\"border-bottom: 1px solid #DDDDDD\">sally</td>\r\n",
        "    <td style=\"border-bottom: 1px solid #DDDDDD; border-right: 1px solid #DDDDDD\">6.4</td>\r\n",
        "  </tr>\r\n",
        "\r\n",
        "</table>\r\n",
        "\r\n",
        "See also\r\n",
        "* <a href=\"https://parquet.apache.org/\" target=\"_blank\">https&#58;//parquet.apache.org</a>\r\n",
        "* <a href=\"https://en.wikipedia.org/wiki/Apache_Parquet\" target=\"_blank\">https&#58;//en.wikipedia.org/wiki/Apache_Parquet</a>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Source\r\n",
        "\r\n",
        "The data for this example shows the number of requests to Wikipedia's mobile and desktop websites (<a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">23 MB</a> from Wikipedia). \r\n",
        "\r\n",
        "The original file, captured August 5th of 2016 was downloaded, converted to a Parquet file and made available for us at **/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/**\r\n",
        "\r\n",
        "Unlike our CSV and JSON example, the parquet \"file\" is actually 11 files, 8 of which consist of the bulk of the data and the other three consist of meta-data. \r\n",
        "\r\n",
        "To read in this files, we will specify the location of the parquet directory."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parquetFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/\"\r\n",
        "\r\n",
        "(spark.read              # The DataFrameReader\r\n",
        "  .parquet(parquetFile)  # Creates a DataFrame from Parquet after reading in the file\r\n",
        "  .printSchema()         # Print the DataFrame's schema\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review: Reading from Parquet Files\r\n",
        "* We do not need to specify the schema - the column names and data types are stored in the parquet files.\r\n",
        "* Only one job is required to **read** that schema from the parquet file's metadata.\r\n",
        "* Unlike the CSV or JSON readers that have to load the entire file and then infer the schema, the parquet reader can \"read\" the schema very quickly because it's reading that schema from the metadata.\r\n",
        "\r\n",
        "If you want to avoid the extra job entirely, we can, again, specify the schema even for parquet files:\r\n",
        "\r\n",
        "** *WARNING* ** *Providing a schema may avoid this one-time hit to determine the `DataFrame's` schema.*  \r\n",
        "*However, if you specify the wrong schema it will conflict with the true schema and will result in an analysis exception at runtime.*\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required for StructField, StringType, IntegerType, etc.\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "parquetSchema = StructType(\r\n",
        "  [\r\n",
        "    StructField(\"timestamp\", StringType(), False),\r\n",
        "    StructField(\"site\", StringType(), False),\r\n",
        "    StructField(\"requests\", IntegerType(), False)\r\n",
        "  ]\r\n",
        ")\r\n",
        "\r\n",
        "(spark.read               # The DataFrameReader\r\n",
        "  .schema(parquetSchema)  # Use the specified schema\r\n",
        "  .parquet(parquetFile)   # Creates a DataFrame from Parquet after reading in the file\r\n",
        "  .printSchema()          # Print the DataFrame's schema\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parquetDF = spark.read.schema(parquetSchema).parquet(parquetFile)\r\n",
        "\r\n",
        "print(\"Partitions: \" + str(parquetDF.rdd.getNumPartitions()) )\r\n",
        "printRecordsPerPartition(parquetDF)\r\n",
        "print(\"-\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most/many cases, people do not provide the schema for Parquet files because reading in the schema is such a cheap process.\r\n",
        "\r\n",
        "And lastly, let's peek at the data:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(parquetDF)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Registering Tables in Databricks\r\n",
        "\r\n",
        "So far we've seen purely programmatic methods for reading in data.\r\n",
        "\r\n",
        "Databricks allows us to \"register\" the equivalent of \"tables\" so that they can be easily accessed by all users. \r\n",
        "\r\n",
        "It also allows us to specify configuration settings such as secret keys, tokens, username & passwords, etc without exposing that information to all users.\r\n",
        "\r\n",
        "## Register a Table/View\r\n",
        "* Databrick's UI has built in support for working with a number of different data sources\r\n",
        "* New ones are being added regularly\r\n",
        "* In our case we are going to upload the file <a href=\"http://files.training.databricks.com/static/data/pageviews_by_second_example.tsv\">pageviews_by_second_example.tsv</a>\r\n",
        "* .. and then use the UI to create a table.\r\n",
        "\r\n",
        "There are several benefits to this strategy:\r\n",
        "* Once setup, it never has to be done again\r\n",
        "* It is available for any user on the platform (permissions permitting)\r\n",
        "* Minimizes exposure of credentials\r\n",
        "* No real overhead to reading the schema (no infer-schema)\r\n",
        "* Easier to advertise available datasets to other users\r\n",
        "\r\n",
        "## Follow these steps to register a new Table\r\n",
        "\r\n",
        "**NOTE:** *It may be easiest for you to duplicate this browser tab so you can refer back to these steps.*\r\n",
        "\r\n",
        "1. Download the [pageviews_by_second_example.tsv](http://files.training.databricks.com/static/data/pageviews_by_second_example.tsv) file to your computer.\r\n",
        "2. Select **Data** in the left-hand menu.\r\n",
        "3. Select the database with your username.\r\n",
        "4. Select **Add Data** to create a new Table.\r\n",
        "\r\n",
        "  ![The Data menu item and Add Data button are both highlighted.](https://databricksdemostore.blob.core.windows.net/images/03-de-learning-path/data-add-data.png)\r\n",
        "\r\n",
        "5. In the Create New Table form, make sure **Upload File** is selected, then click on browse and select the [pageviews_by_second_example.tsv](http://files.training.databricks.com/static/data/pageviews_by_second_example.tsv) file is highlighted, or drag and drop it into the File box.\r\n",
        "6. Select **Create Table with UI**.\r\n",
        "\r\n",
        "  ![The previously listed form options are shown.](https://databricksdemostore.blob.core.windows.net/images/03-de-learning-path/create-new-table-1.png)\r\n",
        "\r\n",
        "7. Select your cluster, then select **Preview Table**.\r\n",
        "8. Under **Create in Database**, select the database with your username in the list. It is **important** that you do not skip this step. You can find the database name in the output of `cell 3` above.\r\n",
        "9. Select **Create Table**.\r\n",
        "\r\n",
        "  ![The previously listed form options are shown.](https://databricksdemostore.blob.core.windows.net/images/03-de-learning-path/create-new-table-2.png)\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pageviewsBySecondsExampleDF = spark.read.table(\"guo20_shernet_sheridancollege_ca_db.pageviews_by_second_example_1_tsv\")\r\n",
        "\r\n",
        "pageviewsBySecondsExampleDF.printSchema()\r\n",
        "\r\n",
        "display(pageviewsBySecondsExampleDF)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review: Reading from Tables\r\n",
        "* No job is executed - the schema is stored in the table definition on Databricks.\r\n",
        "* The data types shown here are those we defined when we registered the table.\r\n",
        "* In our case, the file was uploaded to Databricks and is stored on the DBFS.\r\n",
        "  * If we used JDBC, it would open the connection to the database and read it in.\r\n",
        "  * If we used an object store (like what is backing the DBFS), it would read the data from source.\r\n",
        "* The \"registration\" of the table simply makes future access, or access by multiple users easier.\r\n",
        "* The users of the notebook cannot see username and passwords, secret keys, tokens, etc.\r\n",
        "\r\n",
        "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from a Table/View\r\n",
        "\r\n",
        "We can now read in the \"table\" **pageviews_by_seconds_example** as a `DataFrame` with one simple command (and then print the schema):\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Partitions: \" + str(pageviewsBySecondsExampleDF.rdd.getNumPartitions()))\r\n",
        "printRecordsPerPartition(pageviewsBySecondsExampleDF)\r\n",
        "print(\"-\"*80)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Temporary Views\r\n",
        "\r\n",
        "Tables that are loadable by the call `spark.read.table(..)` are also accessible through the SQL APIs.\r\n",
        "\r\n",
        "For example, we already used Databricks to expose **pageviews_by_second_example_tsv** as a table/view."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "select * from guo20_shernet_sheridancollege_ca_db.pageviews_by_second_example_1_tsv limit(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also take an existing `DataFrame` and register it as a view exposing it as a table to the SQL API.\r\n",
        "\r\n",
        "If you recall from earlier, we have an instance called `parquetDF`.\r\n",
        "\r\n",
        "We can create a [temporary] view with this call..."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a DataFrame from a parquet file\r\n",
        "parquetFile = \"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\r\n",
        "parquetDF = spark.read.parquet(parquetFile)\r\n",
        "\r\n",
        "# create a temporary view from the resulting DataFrame\r\n",
        "parquetDF.createOrReplaceTempView(\"parquet_table\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can use the SQL API to reference that same `DataFrame` as the table **parquet_table**."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "select * from parquet_table order by requests desc limit(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** *Note #1:* ** *The method createOrReplaceTempView(..) is bound to the SparkSession meaning it will be discarded once the session ends.*\r\n",
        "\r\n",
        "** *Note #2:* ** On the other hand, the method createOrReplaceGlobalTempView(..) is bound to the spark application.*\r\n",
        "\r\n",
        "*Or to put that another way, I can use createOrReplaceTempView(..) in this notebook only. However, I can call createOrReplaceGlobalTempView(..) in this notebook and then access it from another.*\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing Data\r\n",
        "\r\n",
        "Just as there are many ways to read data, we have just as many ways to write data.\r\n",
        "\r\n",
        "In this notebook, we will take a quick peek at how to write data back out to Parquet files.\r\n",
        "\r\n",
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Writing Data\r\n",
        "\r\n",
        "Let's start with one of our original CSV data sources, **pageviews_by_second.tsv**:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "csvSchema = StructType([\r\n",
        "  StructField(\"timestamp\", StringType(), False),\r\n",
        "  StructField(\"site\", StringType(), False),\r\n",
        "  StructField(\"requests\", IntegerType(), False)\r\n",
        "])\r\n",
        "\r\n",
        "csvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\r\n",
        "\r\n",
        "csvDF = (spark.read\r\n",
        "  .option('header', 'true')\r\n",
        "  .option('sep', \"\\t\")\r\n",
        "  .schema(csvSchema)\r\n",
        "  .csv(csvFile)\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a `DataFrame`, we can write it back out as Parquet files or other various formats."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileName = userhome + \"/pageviews_by_second.parquet\"\r\n",
        "print(\"Output location: \" + fileName)\r\n",
        "\r\n",
        "(csvDF.write                       # Our DataFrameWriter\r\n",
        "  .option(\"compression\", \"snappy\") # One of none, snappy, gzip, and lzo\r\n",
        "  .mode(\"overwrite\")               # Replace existing files\r\n",
        "  .parquet(fileName)               # Write DataFrame to Parquet files\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the file has been written out, we can see it in the DBFS:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(\r\n",
        "  dbutils.fs.ls(fileName)\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And lastly we can read that same parquet file back in and display the results:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(\r\n",
        "  spark.read.parquet(fileName)\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\r\n",
        "0. Start with the file **dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv**, some random file you haven't seen yet.\r\n",
        "0. Read in the data and assign it to a `DataFrame` named **testDF**.\r\n",
        "0. Run the last cell to verify that the data was loaded correctly and to print its schema.\r\n",
        "0. The one untestable requirement is that you should be able to create the `DataFrame` and print its schema **without** executing a single job.\r\n",
        "\r\n",
        "**Note:** For the test to pass, the following columns should have the specified data types:\r\n",
        " * **prev_id**: integer\r\n",
        " * **curr_id**: integer\r\n",
        " * **n**: integer\r\n",
        " * **prev_title**: string\r\n",
        " * **curr_title**: string\r\n",
        " * **type**: string"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The students will actually need to do this in two steps.\r\n",
        "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\r\n",
        "\r\n",
        "# The first step will be to use inferSchema = true \r\n",
        "# It's the only way to figure out what the column and data types are\r\n",
        "(spark.read\r\n",
        "  .option(\"sep\", \"\\t\")\r\n",
        "  .option(\"header\", \"true\")\r\n",
        "  .option(\"inferSchema\", \"true\")\r\n",
        "  .csv(fileName)\r\n",
        "  .printSchema()\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "# The second step is to create the schema\r\n",
        "schema = StructType([\r\n",
        "    StructField(\"prev_id\", IntegerType(), False),\r\n",
        "    StructField(\"curr_id\", IntegerType(), False),\r\n",
        "    StructField(\"n\", IntegerType(), False),\r\n",
        "    StructField(\"prev_title\", StringType(), False),\r\n",
        "    StructField(\"curr_title\", StringType(), False),\r\n",
        "    StructField(\"type\", StringType(), False)\r\n",
        "])\r\n",
        "\r\n",
        "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\r\n",
        "\r\n",
        "#The third step is to read the data in with the user-defined schema\r\n",
        "testDF = (spark.read\r\n",
        "  .option(\"sep\", \"\\t\")\r\n",
        "  .option(\"header\", \"true\")\r\n",
        "  .schema(schema)\r\n",
        "  .csv(fileName)\r\n",
        ")\r\n",
        "\r\n",
        "testDF.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": "How to read, write and query from DataFram within Spark..",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}