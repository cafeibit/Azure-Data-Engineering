{
	"name": "Work with DataFrames advanced methods",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f955f053-585a-4e28-822c-f696ea1d2f2b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Perform date and time manipulation\r\n",
					"\r\n",
					"** Data Source **\r\n",
					"* English Wikipedia pageviews by second\r\n",
					"* Size on Disk: ~255 MB\r\n",
					"* Type: Parquet files\r\n",
					"* More Info: <a href=\"https://datahub.io/en/dataset/english-wikipedia-pageviews-by-second\" target=\"_blank\">https&#58;//datahub.io/en/dataset/english-wikipedia-pageviews-by-second</a>\r\n",
					"\r\n",
					"**Technical Accomplishments:**\r\n",
					"* Explore more of the `...sql.functions` operations\r\n",
					"  * Date & time functions"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Data Source\r\n",
					"\r\n",
					"This data uses the **Pageviews By Seconds** data set."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.types import *\r\n",
					"\r\n",
					"# I've already gone through the exercise to determine\r\n",
					"# how many partitions I want and in this case it is...\r\n",
					"partitions = 8\r\n",
					"\r\n",
					"# Make sure wide operations don't repartition to 200\r\n",
					"spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(source, sasEntity, sasToken) = getAzureDataSource()\r\n",
					"spark.conf.set(sasEntity, sasToken)\r\n",
					"\r\n",
					"# The directory containing our parquet files.\r\n",
					"parquetFile = source + \"/wikipedia/pageviews/pageviews_by_second.parquet/\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create our initial DataFrame. We can let it infer the \r\n",
					"# schema because the cost for parquet files is really low.\r\n",
					"initialDF = (spark.read\r\n",
					"  .option(\"inferSchema\", \"true\") # The default, but not costly w/Parquet\r\n",
					"  .parquet(parquetFile)          # Read the data in\r\n",
					"  .repartition(partitions)       # From 7 >>> 8 partitions\r\n",
					"  .cache()                       # Cache the expensive operation\r\n",
					")\r\n",
					"# materialize the cache\r\n",
					"initialDF.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Preparing Our Data\r\n",
					"\r\n",
					"If we will be working on any given dataset for a while, there are a handful of \"necessary\" steps to get us ready...\r\n",
					"\r\n",
					"Most of which we've just knocked out above.\r\n",
					"\r\n",
					"**Basic Steps**\r\n",
					"0. <div style=\"text-decoration:line-through\">Read the data in</div>\r\n",
					"0. <div style=\"text-decoration:line-through\">Balance the number of partitions to the number of slots</div>\r\n",
					"0. <div style=\"text-decoration:line-through\">Cache the data</div>\r\n",
					"0. <div style=\"text-decoration:line-through\">Adjust the `spark.sql.shuffle.partitions`</div>\r\n",
					"0. Perform some basic ETL (i.e., convert strings to timestamp)\r\n",
					"0. Possibly re-cache the data if the ETL was costly\r\n",
					"\r\n",
					"What we haven't done is some of the basic ETL necessary to explore our data.\r\n",
					"\r\n",
					"Namely, the problem is that the field \"timestamp\" is a string.\r\n",
					"\r\n",
					"In order to performed date/time - based computation I need to convert this to an alternate datetime format."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"initialDF.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) withColumnRenamed(..), withColumn(..), select(..)\r\n",
					"\r\n",
					"My first hangup is that we have a **column named timestamp** and the **datatype will also be timestamp**\r\n",
					"\r\n",
					"The nice thing about Apache Spark is that I'm allowed the have an issue with this because it's very easy to fix...\r\n",
					"\r\n",
					"Just rename the column..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(initialDF\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"  .printSchema()\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(initialDF\r\n",
					"  .toDF(\"capturedAt\", \"site\", \"requests\")\r\n",
					"  .printSchema()\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) unix_timestamp(..) & cast(..)\r\n",
					"\r\n",
					"Now that **we** are over **my** hangup, we can focus on converting the **string** to a **timestamp**.\r\n",
					"\r\n",
					"For this we will be looking at more of the functions in the `functions` package\r\n",
					"* `pyspark.sql.functions` in the case of Python\r\n",
					"* `org.apache.spark.sql.functions` in the case of Scala & Java\r\n",
					"\r\n",
					"And so that we can watch the transformation, will will take one step at a time..."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The first function is `unix_timestamp(..)`\r\n",
					"\r\n",
					"If you look at the API docs, `unix_timestamp(..)` is described like this:\r\n",
					"> Convert time string with given pattern (see <a href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\" target=\"_blank\">SimpleDateFormat</a>) to Unix time stamp (in seconds), return null if fail.\r\n",
					"\r\n",
					"`SimpleDataFormat` is part of the Java API and provides support for parsing and formatting date and time values.\r\n",
					"\r\n",
					"In order to know what format the data is in, let's take a look at the first row..."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Comparing that value with the patterns express in the docs for the `SimpleDateFormat` class, we can come up with a format:\r\n",
					"\r\n",
					"**yyyy-MM-dd HH:mm:ss**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tempA = (initialDF\r\n",
					"2\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"3\r\n",
					"  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd HH:mm:ss\") )\r\n",
					"4\r\n",
					")\r\n",
					"5\r\n",
					"tempA.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(tempA)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"** *Note:* ** *If you haven't caught it yet, there is a bug in the previous code....*\r\n",
					"\r\n",
					"A couple of things happened...\r\n",
					"0. We ended up with a new column - that's OK for now\r\n",
					"0. The new column has a really funky name - based upon the name of the function we called and its parameters.\r\n",
					"0. The data type is now a long.\r\n",
					"  * This value is the Java Epoch\r\n",
					"  * The number of seconds since 1970-01-01T00:00:00Z\r\n",
					"  \r\n",
					"We can now take that epoch value and use the `Column.cast(..)` method to convert it to a **timestamp**."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tempB = (initialDF\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\r\n",
					")\r\n",
					"tempB.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(tempB)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Now that our column `createdAt` has been converted from a **string** to a **timestamp**, we just need to deal with this REALLY funky column name.\r\n",
					"\r\n",
					"Again.. there are several ways to do this.\r\n",
					"\r\n",
					"I'll let you decide which you like better...\r\n",
					"\r\n",
					"### Option #1\r\n",
					"The `as()` or `alias()` method can be appended to the chain of calls.\r\n",
					"\r\n",
					"This version will actually produce an odd little bug.<br/>\r\n",
					"That is, how do you get rid of only one of the two `capturedAt` columns?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tempC = (initialDF\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\").alias(\"capturedAt\") )\r\n",
					")\r\n",
					"tempC.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(tempC)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Option #2\r\n",
					"The `withColumn(..)` renames the column (first param) and accepts as a<br/>\r\n",
					"second parameter the expression(s) we need for our transformation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tempD = (initialDF\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\r\n",
					")\r\n",
					"tempD.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(tempD)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Option #3\r\n",
					"\r\n",
					"We can take the big ugly name explicitly rename it.\r\n",
					"\r\n",
					"This version will actually produce an odd little bug.<br/>\r\n",
					"That is how do you get rid of only one of the two \"capturedAt\" columns?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Option #3\r\n",
					"\r\n",
					"tempE = (initialDF\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\r\n",
					"  .withColumnRenamed(\"CAST(unix_timestamp(capturedAt, yyyy-MM-dd'T'HH:mm:ss) AS TIMESTAMP)\", \"capturedAt\")\r\n",
					"  # .drop(\"timestamp\")\r\n",
					")\r\n",
					"tempE.printSchema()\r\n",
					"\r\n",
					"display(tempE)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Option #4\r\n",
					"\r\n",
					"The last version is a twist on the others in which we start with the <br/>\r\n",
					"name `timestamp` and rename it and the expression all in one call<br/>\r\n",
					"\r\n",
					"But this version leaves us with the old column in the DF"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tempF = (initialDF\r\n",
					"  .withColumn(\"capturedAt\", unix_timestamp( col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\r\n",
					")\r\n",
					"tempF.printSchema()\r\n",
					"\r\n",
					"display(tempF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Let's pick the \"cleanest\" version...\r\n",
					"\r\n",
					"And with our base `DataFrame` in place we can start exploring the data a little..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pageviewsDF = (initialDF\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\r\n",
					")\r\n",
					"\r\n",
					"pageviewsDF.printSchema()\r\n",
					"\r\n",
					"display(pageviewsDF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"And just so that we don't have to keep performing these transformations.... \r\n",
					"\r\n",
					"Mark the `DataFrame` as cached and then materialize the result."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pageviewsDF.cache().count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) year(..), month(..), dayofyear(..)\r\n",
					"\r\n",
					"Let's take a look at some of the other date & time functions...\r\n",
					"\r\n",
					"With that we can answer a simple question: When was this data captured.\r\n",
					"\r\n",
					"We can start specifically with the year..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(\r\n",
					"  pageviewsDF\r\n",
					"    .select( year( col(\"capturedAt\")) ) # Every record converted to a single column - the year captured\r\n",
					"    .distinct()                         # Reduce all years to the list of distinct years\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Now let's take a look at in which months was this data captured..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(\r\n",
					"  pageviewsDF\r\n",
					"    .select( month( col(\"capturedAt\")) ) # Every record converted to a single column - the month captured\r\n",
					"    .distinct()                          # Reduce all months to the list of distinct months\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"And of course this both can be combined as a single call..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(pageviewsDF\r\n",
					"  .select( month(col(\"capturedAt\")).alias(\"month\"), year(col(\"capturedAt\")).alias(\"year\"))\r\n",
					"  .distinct()\r\n",
					"  .show()                     \r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"It's pretty easy to see that the data was captured during March & April of 2015.\r\n",
					"\r\n",
					"We will have more opportunities to play with the various date and time functions in the next exercise.\r\n",
					"\r\n",
					"For now, let's just make sure to review them in the Spark API"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Use aggregate functions\r\n",
					"\r\n",
					"** Data Source **\r\n",
					"* English Wikipedia pageviews by second\r\n",
					"* Size on Disk: ~255 MB\r\n",
					"* Type: Parquet files\r\n",
					"* More Info: <a href=\"https://datahub.io/en/dataset/english-wikipedia-pageviews-by-second\" target=\"_blank\">https&#58;//datahub.io/en/dataset/english-wikipedia-pageviews-by-second</a>\r\n",
					"\r\n",
					"**Technical Accomplishments:**\r\n",
					"* Introduce the various aggregate functions."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.types import *\r\n",
					"\r\n",
					"# I've already gone through the exercise to determine\r\n",
					"# how many partitions I want and in this case it is...\r\n",
					"partitions = 8\r\n",
					"\r\n",
					"# Make sure wide operations don't repartition to 200\r\n",
					"spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(source, sasEntity, sasToken) = getAzureDataSource()\r\n",
					"spark.conf.set(sasEntity, sasToken)\r\n",
					"\r\n",
					"# The directory containing our parquet files.\r\n",
					"parquetFile = source + \"/wikipedia/pageviews/pageviews_by_second.parquet/\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create our initial DataFrame. We can let it infer the \r\n",
					"2\r\n",
					"# schema because the cost for parquet files is really low.\r\n",
					"3\r\n",
					"initialDF = (spark.read\r\n",
					"4\r\n",
					"  .option(\"inferSchema\", \"true\") # The default, but not costly w/Parquet\r\n",
					"5\r\n",
					"  .parquet(parquetFile)          # Read the data in\r\n",
					"6\r\n",
					"  .repartition(partitions)       # From 7 >>> 8 partitions\r\n",
					"7\r\n",
					"  .cache()                       # Cache the expensive operation\r\n",
					"8\r\n",
					")\r\n",
					"9\r\n",
					"# materialize the cache\r\n",
					"10\r\n",
					"initialDF.count()\r\n",
					"11\r\n",
					" \r\n",
					"12\r\n",
					"# rename the timestamp column and cast to a timestamp data type\r\n",
					"13\r\n",
					"pageviewsDF = (initialDF\r\n",
					"14\r\n",
					"  .withColumnRenamed(\"timestamp\", \"capturedAt\")\r\n",
					"15\r\n",
					"  .withColumn(\"capturedAt\", unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\") )\r\n",
					"16\r\n",
					")\r\n",
					"17\r\n",
					" \r\n",
					"18\r\n",
					"# cache the transformations on our new DataFrame by marking the DataFrame as cached and then materialize the result\r\n",
					"19\r\n",
					"pageviewsDF.cache().count()\r\n",
					"\r\n",
					"display(pageviewsDF)\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) groupBy()\r\n",
					"\r\n",
					"Aggregating data is one of the more common tasks when working with big data.\r\n",
					"* How many customers are over 65?\r\n",
					"* What is the ratio of men to women?\r\n",
					"* Group all emails by their sender.\r\n",
					"\r\n",
					"The function `groupBy()` is one tool that we can use for this purpose.\r\n",
					"\r\n",
					"If you look at the API docs, `groupBy(..)` is described like this:\r\n",
					"> Groups the Dataset using the specified columns, so that we can run aggregation on them.\r\n",
					"\r\n",
					"This function is a **wide** transformation - it will produce a shuffle and conclude a stage boundary.\r\n",
					"\r\n",
					"Unlike all of the other transformations we've seen so far, this transformation does not return a `DataFrame`.\r\n",
					"* In Scala it returns `RelationalGroupedDataset`\r\n",
					"* In Python it returns `GroupedData`\r\n",
					"\r\n",
					"This is because the call `groupBy(..)` is only 1/2 of the transformation.\r\n",
					"\r\n",
					"To see the other half, we need to take a look at it's return type, `RelationalGroupedDataset`."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### RelationalGroupedDataset\r\n",
					"\r\n",
					"If we take a look at the API docs for `RelationalGroupedDataset`, we can see that it supports the following aggregations:\r\n",
					"\r\n",
					"| Method | Description |\r\n",
					"|--------|-------------|\r\n",
					"| `avg(..)` | Compute the mean value for each numeric columns for each group. |\r\n",
					"| `count(..)` | Count the number of rows for each group. |\r\n",
					"| `sum(..)` | Compute the sum for each numeric columns for each group. |\r\n",
					"| `min(..)` | Compute the min value for each numeric column for each group. |\r\n",
					"| `max(..)` | Compute the max value for each numeric columns for each group. |\r\n",
					"| `mean(..)` | Compute the average value for each numeric columns for each group. |\r\n",
					"| `agg(..)` | Compute aggregates by specifying a series of aggregate columns. |\r\n",
					"| `pivot(..)` | Pivots a column of the current DataFrame and perform the specified aggregation. |\r\n",
					"\r\n",
					"With the exception of `pivot(..)`, each of these functions return our new `DataFrame`."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Notice above that we didn't actually specify which column we were summing....\r\n",
					"\r\n",
					"In this case you will actually receive a total for all numerical values.\r\n",
					"\r\n",
					"There is a performance catch to that - if I have 2, 5, 10? columns, then they will all be summed and I may only need one.\r\n",
					"\r\n",
					"I can first reduce my columns to those that I wanted or I can simply specify which column(s) to sum up."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(\r\n",
					"  pageviewsDF\r\n",
					"    .groupBy( col(\"site\") )\r\n",
					"    .sum(\"requests\")\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"And because I don't like the resulting column name, **sum(requests)** I can easily rename it..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(\r\n",
					"  pageviewsDF\r\n",
					"    .groupBy( col(\"site\") )\r\n",
					"    .sum(\"requests\")\r\n",
					"    .withColumnRenamed(\"sum(requests)\", \"totalRequests\")\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"How about the total number of requests per site? mobile vs desktop?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"display(\r\n",
					"  pageviewsDF\r\n",
					"    .groupBy( col(\"site\") )\r\n",
					"    .count()\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This result shouldn't surprise us... there were after all one record, per second, per site....\r\n",
					"\r\n",
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) sum(), count(), avg(), min(), max()\r\n",
					"\r\n",
					"The `groupBy(..)` operation is not our only option for aggregating.\r\n",
					"\r\n",
					"The `...sql.functions` package actually defines a large number of aggregate functions\r\n",
					"* `org.apache.spark.sql.functions` in the case of Scala & Java\r\n",
					"* `pyspark.sql.functions` in the case of Python\r\n",
					"\r\n",
					"\r\n",
					"Let's take a look at this in the Scala API docs (only because the documentation is a little easier to read).\r\n",
					"\r\n",
					"Let's take a look at our last two examples... \r\n",
					"\r\n",
					"We saw the count of records and the sum of records.\r\n",
					"\r\n",
					"Let's take do this a slightly different way...\r\n",
					"\r\n",
					"This time with the `...sql.functions` operations.\r\n",
					"\r\n",
					"And just for fun, let's throw in the average, minimum and maximum"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(pageviewsDF\r\n",
					"  .filter(\"site = 'mobile'\")\r\n",
					"  .select( sum( col(\"requests\")), count(col(\"requests\")), avg(col(\"requests\")), min(col(\"requests\")), max(col(\"requests\")) )\r\n",
					"  .show()\r\n",
					")\r\n",
					"          \r\n",
					"(pageviewsDF\r\n",
					"  .filter(\"site = 'desktop'\")\r\n",
					"  .select( sum( col(\"requests\")), count(col(\"requests\")), avg(col(\"requests\")), min(col(\"requests\")), max(col(\"requests\")) )\r\n",
					"  .show()\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"And let's just address one more pet-peeve...\r\n",
					"\r\n",
					"Was that 3.6M records or 360K records?"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(pageviewsDF\r\n",
					"  .filter(\"site = 'mobile'\")\r\n",
					"  .select( \r\n",
					"    format_number(sum(col(\"requests\")), 0).alias(\"sum\"), \r\n",
					"    format_number(count(col(\"requests\")), 0).alias(\"count\"), \r\n",
					"    format_number(avg(col(\"requests\")), 2).alias(\"avg\"), \r\n",
					"    format_number(min(col(\"requests\")), 0).alias(\"min\"), \r\n",
					"    format_number(max(col(\"requests\")), 0).alias(\"max\") \r\n",
					"  )\r\n",
					"  .show()\r\n",
					")\r\n",
					"\r\n",
					"(pageviewsDF\r\n",
					"  .filter(\"site = 'desktop'\")\r\n",
					"  .select( \r\n",
					"    format_number(sum(col(\"requests\")), 0), \r\n",
					"    format_number(count(col(\"requests\")), 0), \r\n",
					"    format_number(avg(col(\"requests\")), 2), \r\n",
					"    format_number(min(col(\"requests\")), 0), \r\n",
					"    format_number(max(col(\"requests\")), 0) \r\n",
					"  )\r\n",
					"  .show()\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Lab Exercise\r\n",
					"## De-Duping Data\r\n",
					"\r\n",
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\r\n",
					"\r\n",
					"In this exercise, we're doing ETL on a file we've received from some customer. That file contains data about people, including:\r\n",
					"\r\n",
					"* first, middle and last names\r\n",
					"* gender\r\n",
					"* birth date\r\n",
					"* Social Security number\r\n",
					"* salary\r\n",
					"\r\n",
					"But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\r\n",
					"\r\n",
					"* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\"). \r\n",
					"* The Social Security numbers aren't consistent, either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\r\n",
					"\r\n",
					"The name fields are guaranteed to match, if you disregard character case, and the birth dates will also match. (The salaries will match, as well,\r\n",
					"and the Social Security Numbers *would* match, if they were somehow put in the same format).\r\n",
					"\r\n",
					"Your job is to remove the duplicate records. The specific requirements of your job are:\r\n",
					"\r\n",
					"* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\r\n",
					"* Preserve the data format of the columns. For example, if you write the first name column in all lower-case, you haven't met this requirement.\r\n",
					"* Write the result as a Parquet file, as designated by *destFile*.\r\n",
					"* The final Parquet \"file\" must contain 8 part files (8 files ending in \".parquet\").\r\n",
					"\r\n",
					"<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** The initial dataset contains 103,000 records.<br/>\r\n",
					"The de-duplicated result haves 100,000 records."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Hints\r\n",
					"\r\n",
					"* Use the <a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">API docs</a>. Specifically, you might find \r\n",
					"  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\" target=\"_blank\">DataFrame</a> and\r\n",
					"  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">functions</a> to be helpful.\r\n",
					"* It's helpful to look at the file first, so you can check the format. `dbutils.fs.head()` (or just `%fs head`) is a big help here."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"finalDF = spark.read.parquet(destFile)\r\n",
					"finalCount = finalDF.count()\r\n",
					"\r\n",
					"clearYourResults()\r\n",
					"validateYourAnswer(\"01 Expected 100000 Records\", 972882115, finalCount)\r\n",
					"summarizeYourResults()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ANSWER\r\n",
					"\r\n",
					"sourceFile = \"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\r\n",
					"destFile = userhome + \"/people.parquet\"\r\n",
					"\r\n",
					"# In case it already exists\r\n",
					"dbutils.fs.rm(destFile, True)\r\n",
					"\r\n",
					"# First, let's see what the file looks like.\r\n",
					"print(dbutils.fs.head(sourceFile))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ANSWER\r\n",
					"\r\n",
					"# dropDuplicates() will likely introduce a shuffle, so it helps to reduce the number of post-shuffle partitions.\r\n",
					"spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ANSWER\r\n",
					"\r\n",
					"# Okay, now we can read this thing.\r\n",
					"\r\n",
					"df = (spark\r\n",
					"    .read\r\n",
					"    .option(\"header\", \"true\")\r\n",
					"    .option(\"inferSchema\", \"true\")\r\n",
					"    .option(\"sep\", \":\")\r\n",
					"    .csv(sourceFile)\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ANSWER\r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"dedupedDF = (df\r\n",
					"  .select(col(\"*\"),\r\n",
					"      lower(col(\"firstName\")).alias(\"lcFirstName\"),\r\n",
					"      lower(col(\"lastName\")).alias(\"lcLastName\"),\r\n",
					"      lower(col(\"middleName\")).alias(\"lcMiddleName\"),\r\n",
					"      translate(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\r\n",
					"      # regexp_replace(col(\"ssn\"), \"-\", \"\").alias(\"ssnNums\")\r\n",
					"      # regexp_replace(col(\"ssn\"), \"\"\"^(\\d{3})(\\d{2})(\\d{4})$\"\"\", \"$1-$2-$3\").alias(\"ssnNums\")\r\n",
					"   )\r\n",
					"  .dropDuplicates([\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\", \"gender\", \"birthDate\", \"salary\"])\r\n",
					"  .drop(\"lcFirstName\", \"lcMiddleName\", \"lcLastName\", \"ssnNums\")\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ANSWER\r\n",
					"\r\n",
					"# Now we can save the results. We'll also re-read them and count them, just as a final check.\r\n",
					"# Just for fun, we'll use the Snappy compression codec. It's not as compact as Gzip, but it's much faster.\r\n",
					"(dedupedDF.write\r\n",
					"   .mode(\"overwrite\")\r\n",
					"   .option(\"compression\", \"snappy\")\r\n",
					"   .parquet(destFile)\r\n",
					")\r\n",
					"dedupedDF = spark.read.parquet(destFile)\r\n",
					"print(\"Total Records: {0:,}\".format( dedupedDF.count() ))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ANSWER\r\n",
					"\r\n",
					"display( dbutils.fs.ls(destFile) )"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Validate Your Answer\r\n",
					"\r\n",
					"At the bare minimum, we can verify that you wrote the parquet file out to **destFile** and that you have the right number of records.\r\n",
					"\r\n",
					"Running the following cell to confirm your result:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"files = dbutils.fs.ls(destFile)\r\n",
					"display(files)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"finalDF = spark.read.parquet(destFile)\r\n",
					"finalCount = finalDF.count()\r\n",
					"\r\n",
					"clearYourResults()\r\n",
					"validateYourAnswer(\"01 Expected 100000 Records\", 972882115, finalCount)\r\n",
					"summarizeYourResults()\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}