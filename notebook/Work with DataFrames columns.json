{
	"name": "Work with DataFrames columns",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fc379cc6-7f6f-4c6b-b5f7-aeb675ee02b8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#DataFrame Column Class\r\n",
					"\r\n",
					"** Data Source **\r\n",
					"* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\r\n",
					"* Size on Disk: ~23 MB\r\n",
					"* Type: Compressed Parquet File\r\n",
					"* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\r\n",
					"\r\n",
					"**Technical Accomplishments:**\r\n",
					"* Continue exploring the `DataFrame` set of APIs.\r\n",
					"* Introduce the `Column` class"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(source, sasEntity, sasToken) = getAzureDataSource()\r\n",
					"spark.conf.set(sasEntity, sasToken)\r\n",
					"\r\n",
					"parquetFile = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\r\n",
					"\r\n",
					"pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\r\n",
					"  .read                     # Our DataFrameReader\r\n",
					"  .parquet(parquetFile)     # Returns an instance of DataFrame\r\n",
					"  .cache()                  # cache the data\r\n",
					")\r\n",
					"\r\n",
					"print(pagecountsEnAllDF)\r\n",
					"\r\n",
					"total = pagecountsEnAllDF.count()\r\n",
					"\r\n",
					"print(\"Record Count: {0:,}\".format( total ))\r\n",
					"\r\n",
					"display(pagecountsEnAllDF)`\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"As we view the data, we can see that there is no real rhyme or reason as to how the data is sorted.\r\n",
					"  * We cannot even tell if the column **project** is sorted - we are seeing only the first 1,000 of some 2.3 million records.\r\n",
					"  * The column **article** is not sorted as evident by the article **A_Little_Boy_Lost** appearing between a bunch of articles starting with numbers and symbols.\r\n",
					"  * The column **requests** is clearly not sorted.\r\n",
					"  * And our **bytes_served** contains nothing but zeros.\r\n",
					"\r\n",
					"So let's start by sorting our data. In doing this, we can answer the following question:\r\n",
					"\r\n",
					"What are the top 10 most requested articles?\r\n",
					"\r\n",
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) orderBy(..) & sort(..)\r\n",
					"\r\n",
					"If you look at the API docs, `orderBy(..)` is described like this:\r\n",
					"> Returns a new Dataset sorted by the given expressions.\r\n",
					"\r\n",
					"Both `orderBy(..)` and `sort(..)` arrange all the records in the `DataFrame` as specified.\r\n",
					"* Like `distinct()` and `dropDuplicates()`, `sort(..)` and `orderBy(..)` are aliases for each other.\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"  \r\n",
					"  * `sort(..)` appealing to functional programmers.\r\n",
					"  * `orderBy(..)` appealing to developers with an SQL background.\r\n",
					"* Like `orderBy(..)` there are two variants of these two methods:\r\n",
					"  * `orderBy(Column)`\r\n",
					"  * `orderBy(String)`\r\n",
					"  * `sort(Column)`\r\n",
					"  * `sort(String)`\r\n",
					"\r\n",
					"All we need to do now is sort our previous `DataFrame`."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sortedDF = (pagecountsEnAllDF\r\n",
					"  .orderBy(\"requests\")\r\n",
					")\r\n",
					"sortedDF.show(10, False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"As you can see, we are not sorting correctly.\r\n",
					"\r\n",
					"We need to reverse the sort.\r\n",
					"\r\n",
					"One might conclude that we could make a call like this:\r\n",
					"\r\n",
					"`pagecountsEnAllDF.orderBy(\"requests desc\")`\r\n",
					"\r\n",
					"Try it in the cell below:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Uncomment and try this:\r\n",
					"# pagecountsEnAllDF.orderBy(\"requests desc\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Why does this not work?\r\n",
					"* The `DataFrames` API is built upon an SQL engine.\r\n",
					"* There is a lot of familiarity with this API and SQL syntax in general.\r\n",
					"* The problem is that `orderBy(..)` expects the name of the column.\r\n",
					"* What we specified was an SQL expression in the form of **requests desc**.\r\n",
					"* What we need is a way to programmatically express such an expression.\r\n",
					"* This leads us to the second variant, `orderBy(Column)` and more specifically, the class `Column`.\r\n",
					"\r\n",
					"** *Note:* ** *Some of the calls in the `DataFrames` API actually accept SQL expressions.*<br/>\r\n",
					"*While these functions will appear in the docs as `someFunc(String)` it's very*<br>\r\n",
					"*important to thoroughly read and understand what the parameter actually represents.*"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Column Class\r\n",
					"\r\n",
					"The `Column` class is an object that encompasses more than just the name of the column, but also column-level-transformations, such as sorting in a descending order.\r\n",
					"\r\n",
					"The first question to ask is how do I create a `Column` object?\r\n",
					"\r\n",
					"In Scala we have these options:"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"** *Note:* ** *We are showing both the Scala and Python versions below for comparison.*<br/>\r\n",
					"*Make sure to run only the one cell for your notebook's default language (Scala or Python)*"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"\r\n",
					"// Scala & Python both support accessing a column from a known DataFrame\r\n",
					"// Uncomment this if you are using the Scala version of this notebook\r\n",
					"// val columnA = pagecountsEnAllDF(\"requests\")    \r\n",
					"\r\n",
					"// This option is Scala specific, but is arugably the cleanest and easy to read.\r\n",
					"val columnB = $\"requests\"          \r\n",
					"\r\n",
					"// If we import ...sql.functions, we get a couple of more options:\r\n",
					"import org.apache.spark.sql.functions._\r\n",
					"\r\n",
					"// This uses the col(..) function\r\n",
					"val columnC = col(\"requests\")\r\n",
					"\r\n",
					"// This uses the expr(..) function which parses an SQL Expression\r\n",
					"val columnD = expr(\"a + 1\")\r\n",
					"\r\n",
					"// This uses the lit(..) to create a literal (constant) value.\r\n",
					"val columnE = lit(\"abc\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%python\r\n",
					"\r\n",
					"# Scala & Python both support accessing a column from a known DataFrame\r\n",
					"# Uncomment this if you are using the Python version of this notebook\r\n",
					"# columnA = pagecountsEnAllDF[\"requests\"]\r\n",
					"\r\n",
					"# The $\"column-name\" version that works for Scala does not work in Python\r\n",
					"# columnB = $\"requests\"      \r\n",
					"\r\n",
					"# If we import ...sql.functions, we get a couple of more options:\r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"# This uses the col(..) function\r\n",
					"columnC = col(\"requests\")\r\n",
					"\r\n",
					"# This uses the expr(..) function which parses an SQL Expression\r\n",
					"columnD = expr(\"a + 1\")\r\n",
					"\r\n",
					"# This uses the lit(..) to create a literal (constant) value.\r\n",
					"columnE = lit(\"abc\")\r\n",
					"\r\n",
					"# Print the type of each attribute\r\n",
					"print(\"columnC: {}\".format(columnC))\r\n",
					"print(\"columnD: {}\".format(columnD))\r\n",
					"print(\"columnE: {}\".format(columnE))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"In the case of Scala, the cleanest version is the **$\"column-name\"** variant.\r\n",
					"\r\n",
					"In the case of Python, the cleanest version is the **col(\"column-name\")** variant.\r\n",
					"\r\n",
					"So with that, we can now create a `Column` object, and apply the `desc()` operation to it:\r\n",
					"\r\n",
					"** *Note:* ** *We are introducing `...sql.functions` specifically for creating `Column` objects.*<br/>\r\n",
					"*We will be reviewing the multitude of other commands available from this part of the API in future notebooks.*"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"column = col(\"requests\").desc()\r\n",
					"\r\n",
					"# Print the column type\r\n",
					"print(\"column:\", column)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"And now we can piece it all together..."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sortedDescDF = (pagecountsEnAllDF\r\n",
					"  .orderBy( col(\"requests\").desc() )\r\n",
					")  \r\n",
					"sortedDescDF.show(10, False) # The top 10 is good enough for now"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"It should be of no surprise that the **Main_Page** (in both the Wikipedia and Wikimedia projects) is the most requested page.\r\n",
					"\r\n",
					"Followed shortly after that is **Special:Search**, Wikipedia's search page.\r\n",
					"\r\n",
					"And if you consider that this data was captured in the August before the 2016 presidential election, the Trumps will be one of the most requested pages on Wikipedia."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Review Column Class\r\n",
					"\r\n",
					"The `Column` objects provide us a programmatic way to build up SQL-ish expressions.\r\n",
					"\r\n",
					"Besides the `Column.desc()` operation we used above, we have a number of other operations that can be performed on a `Column` object.\r\n",
					"\r\n",
					"Here is a preview of the various functions - we will cover many of these as we progress through the class:\r\n",
					"\r\n",
					"**Column Functions**\r\n",
					"* Various mathematical functions such as add, subtract, multiply & divide\r\n",
					"* Various bitwise operators such as AND, OR & XOR\r\n",
					"* Various null tests such as `isNull()`, `isNotNull()` & `isNaN()`.\r\n",
					"* `as(..)`, `alias(..)` & `name(..)` - Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).\r\n",
					"* `between(..)` - A boolean expression that is evaluated to true if the value of this expression is between the given columns.\r\n",
					"* `cast(..)` & `astype(..)` - Convert the column into type dataType.\r\n",
					"* `asc(..)` - Returns a sort expression based on the ascending order of the given column name.\r\n",
					"* `desc(..)` - Returns a sort expression based on the descending order of the given column name.\r\n",
					"* `startswith(..)` - String starts with.\r\n",
					"* `endswith(..)` - String ends with another string literal.\r\n",
					"* `isin(..)` - A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\r\n",
					"* `like(..)` - SQL like expression\r\n",
					"* `rlike(..)` - SQL RLIKE expression (LIKE with Regex).\r\n",
					"* `substr(..)` - An expression that returns a substring.\r\n",
					"* `when(..)` & `otherwise(..)` - Evaluates a list of conditions and returns one of multiple possible result expressions.\r\n",
					"\r\n",
					"The complete list of functions differs from language to language."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Technical Accomplishments:**\r\n",
					"* Continue exploring the `DataFrame` set of APIs.\r\n",
					"* Continue to work with the `Column` class and introduce the `Row` class\r\n",
					"* Introduce the transformations...\r\n",
					"  * `orderBy(..)`\r\n",
					"  * `sort(..)`\r\n",
					"  * `filter(..)`\r\n",
					"  * `where(..)`\r\n",
					"* Introduce the actions...\r\n",
					"  * `collect()`\r\n",
					"  * `take(n)`\r\n",
					"  * `first()`\r\n",
					"  * `head()`"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"sortedDescDF = (pagecountsEnAllDF\r\n",
					"  .orderBy( col(\"requests\").desc() )\r\n",
					")  \r\n",
					"sortedDescDF.show(10, False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"In looking at the data, we can see multiple Wikipedia projects.\r\n",
					"\r\n",
					"What if we want to look at only the main Wikipedia project, **en**?\r\n",
					"\r\n",
					"For that, we will need to filter out some records."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) filter(..) & where(..)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"If you look at the API docs, filter(..) and where(..) are described like this:\r\n",
					"\r\n",
					"Filters rows using the given condition.\r\n",
					"\r\n",
					"Both filter(..) and where(..) return a new dataset containing only those records for which the specified condition is true.\r\n",
					"\r\n",
					"Like distinct() and dropDuplicates(), filter(..) and where(..) are aliases for each other.\r\n",
					"filter(..) appealing to functional programmers.\r\n",
					"where(..) appealing to developers with an SQL background.\r\n",
					"Like orderBy(..) there are two variants of these two methods:\r\n",
					"filter(Column)\r\n",
					"filter(String)\r\n",
					"where(Column)\r\n",
					"where(String)\r\n",
					"Unlike orderBy(String) which requires a column name, filter(String) and where(String) both expect an SQL expression.\r\n",
					"Let's start by looking at the variant using an SQL expression:"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### filter(..) & where(..) w/SQL Expression"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"whereDF = (sortedDescDF\r\n",
					"  .where( \"project = 'en'\" )\r\n",
					")\r\n",
					"whereDF.show(10, False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Now that we are only looking at the main Wikipedia articles, we get a better picture of the most popular articles on Wikipedia.\r\n",
					"\r\n",
					"Next, let's take a look at the second variant that takes a `Column` object as its first parameter:"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### filter(..) & where(..) w/Column"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"filteredDF = (sortedDescDF\r\n",
					"  .filter( col(\"project\") == \"en\")\r\n",
					")\r\n",
					"filteredDF.show(10, False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### A Scala Issue...\r\n",
					"\r\n",
					"With Python, this is pretty straight forward.\r\n",
					"\r\n",
					"But in Scala... notice anything unusual in that last command?\r\n",
					"\r\n",
					"**Question:** In most every programming language, what is a single equals sign (=) used for?\r\n",
					"\r\n",
					"**Question:** What are two equal signs (==) used for?\r\n",
					"\r\n",
					"**Question:** \r\n",
					"* Considering that transformations are lazy...\r\n",
					"* And the == operator executes now...\r\n",
					"* And `filter(..)` and `where(..)` require us to pass a `Column` object...\r\n",
					"* What would be wrong with `$\"project\" == \"en\"`?\r\n",
					"\r\n",
					"Try it...\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"\r\n",
					"$\"project\" === \"en\""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Let's take a look at the Scala Doc for the `Column` object. </br>\r\n",
					"\r\n",
					"| \"Operator\" | Function |\r\n",
					"|:----------:| -------- |\r\n",
					"| === | Equality test |\r\n",
					"| !== | Deprecated inequality test |\r\n",
					"| =!= | Inequality test |\r\n",
					"| <=> | Null safe equality test |"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### The Solution...\r\n",
					"\r\n",
					"With that behind us, we can clearly **see** the top ten most requested articles.\r\n",
					"\r\n",
					"But what if we need to **programmatically** extract the value of the most requested article's name and its number of requests?\r\n",
					"\r\n",
					"That is to say, how do we get the first record, and from there...\r\n",
					"* the value of the second column, **article**, as a string...\r\n",
					"* the value of the third column, **requests**, as an integer...\r\n",
					"\r\n",
					"Before we proceed, let's apply another filter to get rid of **Main_Page** and anything starting with **Special:** - they're just noise to us."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"articlesDF = (filteredDF\r\n",
					"  .drop(\"bytes_served\")\r\n",
					"  .filter( col(\"article\") != \"Main_Page\")\r\n",
					"  .filter( col(\"article\") != \"-\")\r\n",
					"  .filter( col(\"article\").startswith(\"Special:\") == False)\r\n",
					")\r\n",
					"articlesDF.show(10, False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) first() & head()\r\n",
					"\r\n",
					"If you look at the API docs, both `first(..)` and `head(..)` are described like this:\r\n",
					"> Returns the first row.\r\n",
					"\r\n",
					"Just like `distinct()` & `dropDuplicates()` are aliases for each other, so are `first(..)` and `head(..)`.\r\n",
					"\r\n",
					"However, unlike `distinct()` & `dropDuplicates()` which are **transformations** `first(..)` and `head(..)` are **actions**.\r\n",
					"\r\n",
					"Once all processing is done, these methods return the object backing the first record.\r\n",
					"\r\n",
					"In the case of `DataFrames` (both Scala and Python) that object is a `Row`.\r\n",
					"\r\n",
					"In the case of `Datasets` (the strongly typed version of `DataFrames` in Scala and Java), the object may be a `Row`, a `String`, a `Customer`, a `PendingApplication` or any number of custom objects.\r\n",
					"\r\n",
					"Focusing strictly on the `DataFrame` API for now, let's take a look at a call with `head()`:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"firstRow = articlesDF.first()\r\n",
					"\r\n",
					"print(firstRow)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Row Class\r\n",
					"\r\n",
					"Now that we have a reference to the object backing the first row (or any row), we can use it to extract the data for each column.\r\n",
					"\r\n",
					"Before we do, let's take a look at the API docs for the `Row` class.\r\n",
					"\r\n",
					"At the heart of it, we are simply going to ask for the value of the object in column N via `Row.get(i)`.\r\n",
					"\r\n",
					"Python being a loosely typed language, the return value is of no real consequence.\r\n",
					"\r\n",
					"However, Scala is going to return an object of type `Any`. In Java, this would be an object of type `Object`.\r\n",
					"\r\n",
					"What we need (at least for Scala), especially if the data type matters in cases of performing mathematical operations on the value, we need to call one of the other methods:\r\n",
					"* `getAs[T](i):T`\r\n",
					"* `getDate(i):Date`\r\n",
					"* `getString(i):String`\r\n",
					"* `getInt(i):Int`\r\n",
					"* `getLong(i):Long`\r\n",
					"\r\n",
					"We can now put it all together to get the number of requests for the most requested project:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"article = firstRow['article']\r\n",
					"total = firstRow['requests']\r\n",
					"\r\n",
					"print(\"Most Requested Article: \\\"{0}\\\" with {1:,} requests\".format( article, total ))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) collect()\r\n",
					"\r\n",
					"If you look at the API docs, `collect(..)` is described like this:\r\n",
					"> Returns an array that contains all of Rows in this Dataset.\r\n",
					"\r\n",
					"`collect()` returns a collection of the specific type backing each record of the `DataFrame`.\r\n",
					"* In the case of Python, this is always the `Row` object.\r\n",
					"* In the case of Scala, this is also a `Row` object.\r\n",
					"* If the `DataFrame` was converted to a `Dataset` the backing object would be the user-specified object.\r\n",
					"\r\n",
					"Building on our last example, let's take the top 10 records and print them out."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rows = (articlesDF\r\n",
					"  .limit(10)           # We only want the first 10 records.\r\n",
					"  .collect()           # The action returning all records in the DataFrame\r\n",
					")\r\n",
					"\r\n",
					"# rows is an Array. Now in the driver, \r\n",
					"# we can just loop over the array and print 'em out.\r\n",
					"\r\n",
					"listItems = \"\"\r\n",
					"for row in rows:\r\n",
					"  project = row['article']\r\n",
					"  total = row['requests']\r\n",
					"  listItems += \"    <li><b>{}</b> {:0,d} requests</li>\\n\".format(project, total)\r\n",
					"  \r\n",
					"html = \"\"\"\r\n",
					"<body>\r\n",
					"  <h1>Top 10 Articles</h1>\r\n",
					"  <ol>\r\n",
					"    %s\r\n",
					"  </ol>\r\n",
					"</body>\r\n",
					"\"\"\" % (listItems.strip())\r\n",
					"\r\n",
					"print(html)\r\n",
					"\r\n",
					"# UNCOMMENT FOR A PRETTIER PRESENTATION\r\n",
					"# displayHTML(html)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) take(n)\r\n",
					"\r\n",
					"If you look at the API docs, `take(n)` is described like this:\r\n",
					"> Returns the first n rows in the Dataset.\r\n",
					"\r\n",
					"`take(n)` returns a collection of the first N records of the specific type backing each record of the `DataFrame`.\r\n",
					"* In the case of Python, this is always the `Row` object.\r\n",
					"* In the case of Scala, this is also a `Row` object.\r\n",
					"* If the `DataFrame` was converted to a `Dataset` the backing object would be the user-specified object.\r\n",
					"\r\n",
					"In short, it's the same basic function as `collect()` except you specify as the first parameter the number of records to return."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rows = articlesDF.take(10)\r\n",
					"\r\n",
					"# rows is an Array. Now in the driver, \r\n",
					"# we can just loop over the array and print 'em out.\r\n",
					"\r\n",
					"listItems = \"\"\r\n",
					"for row in rows:\r\n",
					"  project = row['article']\r\n",
					"  total = row['requests']\r\n",
					"  listItems += \"    <li><b>{}</b> {:0,d} requests</li>\\n\".format(project, total)\r\n",
					"  \r\n",
					"html = \"\"\"\r\n",
					"<body>\r\n",
					"  <h1>Top 10 Articles</h1>\r\n",
					"  <ol>\r\n",
					"    %s\r\n",
					"  </ol>\r\n",
					"</body>\r\n",
					"\"\"\" % (listItems.strip())\r\n",
					"\r\n",
					"print(html)\r\n",
					"\r\n",
					"# UNCOMMENT FOR A PRETTIER PRESENTATION\r\n",
					"# displayHTML(html)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) DataFrame vs Dataset\r\n",
					"\r\n",
					"We've been alluding to `Datasets` off and on. \r\n",
					"\r\n",
					"The following example demonstrates how to convert a `DataFrame` to a `Dataset`.\r\n",
					"\r\n",
					"And when compared to the previous example, helps to illustrate the difference/relationship between the two.\r\n",
					"\r\n",
					"** *Note:* ** *As a reminder, `Datasets` are a Java and Scala concept and brings to those languages the type safety that *<br/>\r\n",
					"*is lost with `DataFrame`, or rather, `Dataset[Row]`. Python and R have no such concept because they are loosely typed.*\r\n",
					"\r\n",
					"Before we demonstrate this, let's review all our transformations:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"val (source, sasEntity, sasToken) = getAzureDataSource()\r\n",
					"spark.conf.set(sasEntity, sasToken)\r\n",
					"\r\n",
					"val parquetFile = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"\r\n",
					"val articlesDF = spark                          // Our SparkSession & Entry Point\r\n",
					"  .read                                         // Our DataFrameReader\r\n",
					"  .parquet(parquetFile)                         // Creates a DataFrame from a parquet file\r\n",
					"  .filter( $\"project\" === \"en\")                 // Include only the \"en\" project\r\n",
					"  .filter($\"article\" =!= \"Main_Page\")           // Exclude the Wikipedia Main Page\r\n",
					"  .filter($\"article\" =!= \"-\")                   // Exclude some \"weird\" article\r\n",
					"  .filter( ! $\"article\".startsWith(\"Special:\")) // Exclude all the \"special\" articles\r\n",
					"  .drop(\"bytes_served\")                         // We just don't need this column\r\n",
					"  .orderBy( $\"requests\".desc )                  // Sort by requests descending"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Notice above that `articlesDF` is a `Dataset` of type `Row`.\r\n",
					"\r\n",
					"Next, create the case class `WikiReq`. \r\n",
					"\r\n",
					"A little later we can convert this `DataFrame` to a `Dataset` of type `WikiReq`:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"\r\n",
					"// the name and data type of the case class must match the schema they will be converted from.\r\n",
					"case class WikiReq (project:String, article:String, requests:Int)\r\n",
					"\r\n",
					"articlesDF.printSchema"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Instead of the `Row` object, we can now back each record with our new `WikiReq` class.\r\n",
					"\r\n",
					"And we can see the conversion from `DataFrames` to `Datasets` here:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"\r\n",
					"val articlesDS = articlesDF.as[WikiReq]"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Make note of the data type: **org.apache.spark.sql.Dataset[WikiReq]**\r\n",
					"\r\n",
					"Compare that to a `DataFrame`: **org.apache.spark.sql.Dataset[Row]**\r\n",
					"\r\n",
					"Now when we ask for the first 10, we won't get an array of `Row` objects but instead an array of `WikiReq` objects:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%scala\r\n",
					"val wikiReqs = articlesDS.take(10)\r\n",
					"\r\n",
					"// wikiReqs is an Array of WikiReqs. Now in the driver, \r\n",
					"// we can just loop over the array and print 'em out.\r\n",
					"\r\n",
					"var listItems = \"\"\r\n",
					"for (wikiReq <- wikiReqs) {\r\n",
					"  // Notice how we don't relaly need temp variables?\r\n",
					"  // Or more specifically, we don't need to cast.\r\n",
					"  listItems += \"    <li><b>%s</b> %,d requests</li>%n\".format(wikiReq.article, wikiReq.requests)\r\n",
					"}\r\n",
					"\r\n",
					"var html = s\"\"\"\r\n",
					"<body>\r\n",
					"  <h1>Top 10 Articles</h1>\r\n",
					"  <ol>\r\n",
					"    ${listItems.trim()}\r\n",
					"  </ol>\r\n",
					"</body>\r\n",
					"\"\"\"\r\n",
					"\r\n",
					"println(html)\r\n",
					"println(\"-\"*80)\r\n",
					"\r\n",
					"// UNCOMMENT FOR A PRETTIER PRESENTATION\r\n",
					"// displayHTML(html)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/wiki-book/general/logo_spark_tiny.png) Instructions\r\n",
					"\r\n",
					"This data was captured in the August before the 2016 US presidential election.\r\n",
					"\r\n",
					"As a result, articles about the candidates were very popular.\r\n",
					"\r\n",
					"For this exercise, you will...\r\n",
					"0. Filter the result to the **en** Wikipedia project.\r\n",
					"0. Find all the articles where the name of the article **ends** with **_Washington** (presumably \"George Washington\", \"Martha Washington\", etc)\r\n",
					"0. Return all records as an array to the Driver.\r\n",
					"0. Assign your array of Washingtons (the return value of your action) to the variable `washingtons`.\r\n",
					"0. Calculate the sum of requests for the Washingtons and assign it to the variable `totalWashingtons`. <br/>\r\n",
					"<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** We've not yet covered `DataFrame` aggregation techniques, so for this exercise use the array of records you have just obtained.\r\n",
					"\r\n",
					"** Bonus **\r\n",
					"\r\n",
					"Repeat the exercise for the Marthas\r\n",
					"0. Filter the result to the **en** Wikipedia project.\r\n",
					"0. Find all the articles where the name of the article **starts** with **Martha_** (presumably \"Martha Washington\", \"Martha Graham\", etc)\r\n",
					"0. Return all records as an array to the Driver.\r\n",
					"0. Assign your array of Marthas (the return value of your action) to the variable `marthas`.\r\n",
					"0. Calculate the sum of requests for the Marthas and assign it to the variable `totalMarthas`.<br/>\r\n",
					"<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** We've not yet covered `DataFrame` aggregation techniques, so for this exercise use the array of records you have just obtained.\r\n",
					"0. But you cannot do it the same way twice:\r\n",
					"   * In the filter, don't use the same conditional method as the one used for the Washingtons.\r\n",
					"   * Don't use the same action as used for the Washingtons.\r\n",
					"\r\n",
					"**Testing**\r\n",
					"\r\n",
					"Run the last cell to verify that your results are correct.\r\n",
					"\r\n",
					"**Hints**\r\n",
					"* <img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Make sure to include the underscore in the condition.\r\n",
					"* The actions we've explored for extracting data include:\r\n",
					"  * `first()`\r\n",
					"  * `collect()`\r\n",
					"  * `head()`\r\n",
					"  * `take(n)`\r\n",
					"* The conditional methods used with a `filter(..)` include:\r\n",
					"  * equals\r\n",
					"  * not-equals\r\n",
					"  * starts-with\r\n",
					"  * and there are others - remember, the `DataFrames` API is built upon an SQL engine.\r\n",
					"* There shouldn't be more than 1000 records for either the Washingtons or the Marthas"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/wiki-book/general/logo_spark_tiny.png) Show Your Work"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"(source, sasEntity, sasToken) = getAzureDataSource()\r\n",
					"spark.conf.set(sasEntity, sasToken)\r\n",
					"\r\n",
					"parquetDir = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# TODO\r\n",
					"\r\n",
					"# Replace FILL_IN with your code. You will probably need multiple\r\n",
					"# lines of code for this problem.\r\n",
					"\r\n",
					"washingtons = FILL_IN\r\n",
					"\r\n",
					"totalWashingtons = 0\r\n",
					"\r\n",
					"for washington in washingtons:\r\n",
					"  totalWashingtons += FILL_IN\r\n",
					"  \r\n",
					"print(\"Total Washingtons: {0:,}\".format( len(washingtons) ))\r\n",
					"print(\"Total Washington Requests: {0:,}\".format( totalWashingtons ))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# TODO\r\n",
					"\r\n",
					"# Replace FILL_IN with your code. You will probably need multiple\r\n",
					"# lines of code for this problem.\r\n",
					"\r\n",
					"marthas = FILL_IN\r\n",
					"\r\n",
					"totalMarthas = 0\r\n",
					"\r\n",
					"for martha in marthas:\r\n",
					"  totalMarthas += FILL_IN\r\n",
					"\r\n",
					"print(\"Total Marthas: {0:,}\".format( len(marthas) ))\r\n",
					"print(\"Total Martha Requests: {0:,}\".format( totalMarthas ))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/wiki-book/general/logo_spark_tiny.png) Verify Your Work\r\n",
					"Run the following cell to verify that your `DataFrame` was created properly."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(\"Total Washingtons: {0:,}\".format( len(washingtons) ))\r\n",
					"print(\"Total Washington Requests: {0:,}\".format( totalWashingtons ))\r\n",
					"\r\n",
					"expectedCount = 466\r\n",
					"assert len(washingtons) == expectedCount, \"Expected \" + str(expectedCount) + \" articles but found \" + str( len(washingtons) )\r\n",
					"\r\n",
					"expectedTotal = 3266\r\n",
					"assert totalWashingtons == expectedTotal, \"Expected \" + str(expectedTotal) + \" requests but found \" + str(totalWashingtons)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(\"Total Marthas: {0:,}\".format( len(marthas) ))\r\n",
					"print(\"Total Marthas Requests: {0:,}\".format( totalMarthas ))\r\n",
					"\r\n",
					"expectedCount = 146\r\n",
					"assert len(marthas) == expectedCount, \"Expected \" + str(expectedCount) + \" articles but found \" + str( len(marthas) )\r\n",
					"\r\n",
					"expectedTotal = 708\r\n",
					"assert totalMarthas == expectedTotal, \"Expected \" + str(expectedTotal) + \" requests but found \" + str(totalMarthas)\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}