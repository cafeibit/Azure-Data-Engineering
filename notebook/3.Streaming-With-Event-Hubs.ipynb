{"cells":[{"cell_type":"markdown","source":["# Structured Streaming with Azure EventHubs \n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Establish a connection with Event Hubs in Spark\n* Subscribe to and configure an Event Hubs stream\n* Parse JSON records from Event Hubs\n\n## Library Requirements\n\nThe Maven library with coordinate `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18`\n\n## Resources\n- [Docs for Azure Event Hubs connector](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs)\n- [Documentation on how to install Maven libraries](https://docs.azuredatabricks.net/user-guide/libraries.html#maven-or-spark-package)\n- [Spark-EventHub debugging FAQ](https://github.com/Azure/azure-event-hubs-spark/blob/master/FAQ.md)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3bdce63-c791-4a97-8c57-0aa469386c44"}}},{"cell_type":"markdown","source":["## Lab Setup\n\nTo use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Clusters**, then select your cluster in the list. If it's not running, start it now.\n\n  ![Select cluster](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/select-cluster.png)\n\n2. Select the **Libraries** tab (1), then select **Install New** (2). In the Install Library dialog, select **Maven** under Library Source (3). Under Coordinates, paste **com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18** (4), then select **Install**.\n  \n  ![Databricks new Maven library](https://raw.githubusercontent.com/MicrosoftDocs/mslearn_databricks/main/images/install-eventhubs-spark-library.png)\n\n3. Wait until the library successfully installs before continuing.\n\n  ![Library installed](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/eventhubs-spark-library-installed.png)\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2192d1c4-dd57-4bd6-a34c-f9c9100769e9"}}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our classroom and set up a local streaming file read that we'll be writing to Event Hubs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e1ee27e-4299-4082-b6ea-f0a63a9845c1"}}},{"cell_type":"code","source":["%run ./Includes/Streaming-Demo-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2d7cc49-aade-4d49-acb6-f39748776439"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The execution of this command did not finish successfully","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Azure Event Hubs</h2>\n\nMicrosoft Azure Event Hubs is a fully managed, real-time data ingestion service.\nYou can stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\nIt integrates seamlessly with a host of other Azure services.\n\nEvent Hubs can be used in a variety of applications such as\n* Anomaly detection (fraud/outliers)\n* Application logging\n* Analytics pipelines, such as clickstreams\n* Archiving data\n* Transaction processing\n* User telemetry processing\n* Device telemetry streaming\n* <b>Live dashboarding</b>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c215622-ffa3-40a1-9287-6e157a5196fe"}}},{"cell_type":"markdown","source":["### Define Connection Strings and Create Configuration Object\n\nThis cell uses a connection string to create a simple `EventHubsConf` object, which will be used to connect.\n\nTo run this notebook, you'll need to configure Event Hubs and provide the relavent information in the following format:\n```\nEndpoint=sb://<event_hubs_namespace>.servicebus.windows.net/;SharedAccessKeyName=<key_name>;SharedAccessKey=<signing_key>=;EntityPath=<event_hubs_instance>\n```\n\nNote that during the setup steps prior to this noteobok, you were instructed to copy the `Connect string-primary key`; you will need to append the EntityPath with the name of your Event Hub instance to that copied string to successfully connect."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48746d4d-1af0-4850-8a15-7469e02093f1"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.eventhubs.{EventHubsConf, EventPosition}\n\nval connectionString = \"Endpoint=sb://<event_hubs_namespace>.servicebus.windows.net/;SharedAccessKeyName=<key_name>;SharedAccessKey=<signing_key>=;EntityPath=<event_hubs_instance>\"\n\nval ehWriteConf = EventHubsConf(connectionString)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9debc9d5-b4f1-4166-954b-578dbce17329"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Write Stream to Event Hub to Produce Stream\n\nBelow, we configure a streaming write to Event Hubs. Refer to the docs for additional ways to [write data to Event Hubs](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#writing-data-to-eventhubs)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1231f73b-8396-491d-98fc-935baf3928d2"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval checkpointPath = userhome + \"/event-hub/write-checkpoint\"\ndbutils.fs.rm(checkpointPath,true)\n\nactivityStreamDF\n  .writeStream\n  .format(\"eventhubs\")\n  .outputMode(\"update\")\n  .options(ehWriteConf.toMap)\n  .trigger(ProcessingTime(\"25 seconds\"))\n  .option(\"checkpointLocation\", checkpointPath)\n  .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e06edbaf-a343-4d96-95ab-0eef856f5169"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Event Hubs Configuration\n\nAbove, a simple `EventHubsConf` object is used to write data. There are [numerous additional options for configuration](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#eventhubsconf). Below, we specify an `EventPosition` ([docs](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs#eventposition)) and limit our throughput by setting `MaxEventsPerTrigger`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"288994cb-b243-4ba9-8f79-dbe47ed15014"}}},{"cell_type":"code","source":["%scala\n\nval eventHubsConf = EventHubsConf(connectionString)\n  .setStartingPosition(EventPosition.fromStartOfStream)\n  .setMaxEventsPerTrigger(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66f7b70d-328f-4ce3-a141-2154ebdfebb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### READ Stream using EventHub\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4c27ed6-8476-4400-82c7-e15a215a6b74"}}},{"cell_type":"code","source":["%scala\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nval eventStreamDF = spark.readStream\n  .format(\"eventhubs\")\n  .options(eventHubsConf.toMap)\n  .load()\n\neventStreamDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17f29914-df7c-475c-954b-fbe438516d63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Most of the fields in this response are metadata describing the state of the Event Hubs stream. We are specifically interested in the `body` field, which contains our JSON payload.\n\nNoting that it's encoded as binary, as we select it, we'll cast it to a string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38cd31d5-1bf1-495f-965a-34d5e98373f0"}}},{"cell_type":"code","source":["%scala\nval bodyDF = eventStreamDF.select('body.cast(\"STRING\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94e84ac5-e2dd-44f0-bae2-f0615fc0e289"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Each line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\nNotice that nothing happens until you engage an action, i.e. a `display()` or `writeStream`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a2c688d-07c9-4ecf-b976-e659ce321640"}}},{"cell_type":"code","source":["%scala\ndisplay(bodyDF, streamName= \"bodyDF\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e08432f1-f907-40e1-b1e2-9baa46a8012a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["While we can see our JSON data now that it's cast to string type, we can't directly manipulate it.\n\nBefore proceeding, stop this stream. We'll continue building up transformations against this streaming DataFrame, and a new action will trigger an additional stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"574baea1-385d-42a2-bd78-f8e435330009"}}},{"cell_type":"code","source":["%scala\nfor (s <- spark.streams.active if s.name == \"bodyDF\") s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbda01a6-3195-43d3-92ae-776e33d0a984"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## <img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Parse the JSON payload\n\nThe EventHub acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data in the JSON format.\n\nIf desired, we could save this as raw bytes or strings and parse these records further downstream in our processing.\n\nHere, we'll directly parse our data so we can interact with the fields.\n\nThe first step is to define the schema for the JSON payload.\n\n:SIDENOTE: Both time fields are encoded as `LongType` here because of non-standard formatting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70f7f033-7aef-4e7a-ab21-25b62654bc87"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType, DoubleType}\n\nlazy val schema = StructType(List(\n  StructField(\"Arrival_Time\", LongType),\n  StructField(\"Creation_Time\", LongType),\n  StructField(\"Device\", StringType),\n  StructField(\"Index\", LongType),\n  StructField(\"Model\", StringType),\n  StructField(\"User\", StringType),\n  StructField(\"gt\", StringType),\n  StructField(\"x\", DoubleType),\n  StructField(\"y\", DoubleType),\n  StructField(\"z\", DoubleType),\n  StructField(\"geolocation\", StructType(List(\n    StructField(\"PostalCode\", StringType),\n    StructField(\"StateProvince\", StringType),\n    StructField(\"city\", StringType),\n    StructField(\"country\", StringType)))),\n  StructField(\"id\", StringType)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f8500e3-078b-4efd-8af0-4c127d9e655b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Parse the data\n\nNext we can use the function `from_json` to parse out the full message with the schema specified above.\n\nWhen parsing a value from JSON, we end up with a single column containing a complex object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbc902c9-a8fa-4c33-afdf-90ec12fcf181"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.from_json\n\nval parsedEventsDF = bodyDF.select(\n  from_json('body, schema).alias(\"json\"))\n\nparsedEventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b49ade5-ad09-4123-9ae1-56f3cf754cf1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that we can further parse this to flatten the schema entirely and properly cast our time fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed261c8c-91cf-4a03-abff-578b192946b7"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.{from_unixtime, col}\n\nval flatSchemaDF = parsedEventsDF\n  .select(from_unixtime(col(\"json.Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"json.Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"json.Device\").alias(\"Device\"),\n          col(\"json.Index\").alias(\"Index\"),\n          col(\"json.Model\").alias(\"Model\"),\n          col(\"json.User\").alias(\"User\"),\n          col(\"json.gt\").alias(\"gt\"),\n          col(\"json.x\").alias(\"x\"),\n          col(\"json.y\").alias(\"y\"),\n          col(\"json.z\").alias(\"z\"),\n          col(\"json.id\").alias(\"id\"),\n          col(\"json.geolocation.country\").alias(\"country\"),\n          col(\"json.geolocation.city\").alias(\"city\"),\n          col(\"json.geolocation.PostalCode\").alias(\"PostalCode\"),\n          col(\"json.geolocation.StateProvince\").alias(\"StateProvince\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adaa9f12-4cd0-4d47-bd1e-0435e4ec3336"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["This flat schema provides us the ability to view each nested field as a column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b985c41-66ec-4422-9122-cc3b37c10361"}}},{"cell_type":"code","source":["%scala\ndisplay(flatSchemaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22f283c4-1b4e-4105-a0bb-c03b04a4e75e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Stop all active streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbd91863-1215-44eb-9565-e5709ea12cc8"}}},{"cell_type":"code","source":["%scala\nfor (s <- spark.streams.active)\n  s.stop"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97c8f0d5-3d89-4a68-8b60-ad42e7b6d9b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3.Streaming-With-Event-Hubs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1717093603756557}},"nbformat":4,"nbformat_minor":0}
