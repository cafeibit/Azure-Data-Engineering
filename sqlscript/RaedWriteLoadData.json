{
	"name": "RaedWriteLoadData",
	"properties": {
		"content": {
			"query": "\n--Read from the Customer Table by Spark Scala within Databricks Notebooks\n--Next, use the Synapse Connector to read data from the Customer Table.\n--Use the read to define a tempory table that can be queried.\n\n%%spark\ncacheDir = f\"wasbs://{containerName}@{storageAccount}.blob.core.windows.net/cacheDir\"\n\ntableName = \"dbo.DimCustomer\"\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")\n\n%%sql\nselect count(*) from customer_data\n\ndescribe customer_data\n\nselect CustomerKey, CustomerAlternateKey \nfrom customer_data \nlimit 10;\n\n\n--In a situation in which we may be merging many new customers into this table,\n--we can imagine that we may have issues with uniqueness with regard to the `CustomerKey`.\n--Let us redefine `CustomerAlternateKey` for stronger uniqueness using a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).\n--To do this we will define a UDF and use it to transform the `CustomerAlternateKey` column. \n--Once this is done, we will write the updated Customer Table to a Staging table.\n\n%%spark\n\nimport uuid\n\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)\n\n\n--Use the Polybase Connector to Write to the Staging Table\n%%spark\n\n(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forward_spark_azure_storage_credentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())\n\n\n--Read and Display Changes from Staging Table\n\n%%spark\n\ncustomerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")\n\n%%sql\nselect CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;\n\n\n\n--Use data loading best practices in Azure Synapse Analytics \n-- Simplify ingestion with the Copy Activity\n\n-- Replace YOURACCOUNT with the name of your ADLS Gen2 account.\n\nCREATE EXTERNAL DATA SOURCE ABSS\nWITH\n( TYPE = HADOOP,\n    LOCATION = 'abfss://wwi-02@YOURACCOUNT.dfs.core.windows.net'\n);\n\nCREATE EXTERNAL FILE FORMAT [ParquetFormat]\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n-- To create the external file format and external data table.\n\nCREATE SCHEMA [wwi_external];\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].Sales\n    (\n        [TransactionId] [nvarchar](36)  NOT NULL,\n        [CustomerId] [int]  NOT NULL,\n        [ProductId] [smallint]  NOT NULL,\n        [Quantity] [smallint]  NOT NULL,\n        [Price] [decimal](9,2)  NOT NULL,\n        [TotalAmount] [decimal](9,2)  NOT NULL,\n        [TransactionDate] [int]  NOT NULL,\n        [ProfitAmount] [decimal](9,2)  NOT NULL,\n        [Hour] [tinyint]  NOT NULL,\n        [Minute] [tinyint]  NOT NULL,\n        [StoreId] [smallint]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/sale-small%2FYear%3D2019',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = [ParquetFormat]  \n    )  \nGO\n\n--\n\nINSERT INTO [wwi_staging].[SaleHeap]\nSELECT *\nFROM [wwi_external].[Sales]\n\n-- Simplify ingestion with the COPY activity\n\n\n\n\n--Set-up dedicated data load accounts within Azure Synapse Analytics to\n--optimize load performance and maintain concurrency as required \n--by managing the available resource slots available within the dedicated SQL Pool.\n\n-- Connect to master\nCREATE LOGIN loader WITH PASSWORD = 'a123STRONGpassword!';\n\n-- Connect to the SQL pool\nCREATE USER loader FOR LOGIN loader;\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO loader;\nGRANT INSERT ON <yourtablename> TO loader;\nGRANT SELECT ON <yourtablename> TO loader;\nGRANT CREATE TABLE TO loader;\nGRANT ALTER ON SCHEMA::dbo TO loader;\n\nCREATE WORKLOAD GROUP DataLoads\nWITH ( \n    MIN_PERCENTAGE_RESOURCE = 100\n    ,CAP_PERCENTAGE_RESOURCE = 100\n    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 100\n    );\n\nCREATE WORKLOAD CLASSIFIER [wgcELTLogin]\nWITH (\n        WORKLOAD_GROUP = 'DataLoads'\n    ,MEMBERNAME = 'loader'\n);\n\n--Exercise - implement workload management\n--Create a workload classifier to add importance to certain queries\n--Lab 08 - Execute Data Analyst and CEO Queries\n----First, let's confirm that there are no queries currently being run by users logged in workload01 or workload02\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, \nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') \n--and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,s.login_name\n\n--To see what happened to all the queries we just triggered as they flood the system.\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status\n\n--To give our asa.sql.workload01 user queries priority by implementing the Workload Importance feature.\n\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')\nBEGIN\n    DROP WORKLOAD CLASSIFIER CEO;\nEND\nCREATE WORKLOAD CLASSIFIER CEO\n  WITH (WORKLOAD_GROUP = 'largerc'\n  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);\n\n--\n\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')\nBEGIN\n    DROP WORKLOAD CLASSIFIER CEO;\nEND\nCREATE WORKLOAD CLASSIFIER CEO\n  WITH (WORKLOAD_GROUP = 'largerc'\n  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status desc\n\n--Reserve resources for specific workloads through workload isolation\n\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_groups where name = 'CEODemo')\nBEGIN\n    Create WORKLOAD GROUP CEODemo WITH  \n    ( MIN_PERCENTAGE_RESOURCE = 50        -- integer value\n    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25 --  \n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\nEND\n\n--\n\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where  name = 'CEODreamDemo')\nBEGIN\n    Create Workload Classifier CEODreamDemo with\n    ( Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n--IF  EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where group_name = 'CEODemo')\nBEGIN\n    Drop Workload Classifier CEODreamDemo\n    DROP WORKLOAD GROUP CEODemo\n    --- Creates a workload group 'CEODemo'.\n        Create  WORKLOAD GROUP CEODemo WITH  \n    (MIN_PERCENTAGE_RESOURCE = 26 -- integer value\n        ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 3.25 -- factor of 26 (guaranteed more than 8 concurrencies)\n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\n    --- Creates a workload Classifier 'CEODreamDemo'.\n    Create Workload Classifier CEODreamDemo with\n    (Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis  not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "CustomerDatabase",
				"poolName": "Built-in"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}