{
	"name": "RaedWriteLoadData",
	"properties": {
		"content": {
			"query": "\n--Read from the Customer Table by Spark Scala within Databricks Notebooks\n--Next, use the Synapse Connector to read data from the Customer Table.\n--Use the read to define a tempory table that can be queried.\n\n%%spark\ncacheDir = f\"wasbs://{containerName}@{storageAccount}.blob.core.windows.net/cacheDir\"\n\ntableName = \"dbo.DimCustomer\"\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")\n\n%%sql\nselect count(*) from customer_data\n\ndescribe customer_data\n\nselect CustomerKey, CustomerAlternateKey \nfrom customer_data \nlimit 10;\n\n\n--In a situation in which we may be merging many new customers into this table,\n--we can imagine that we may have issues with uniqueness with regard to the `CustomerKey`.\n--Let us redefine `CustomerAlternateKey` for stronger uniqueness using a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).\n--To do this we will define a UDF and use it to transform the `CustomerAlternateKey` column. \n--Once this is done, we will write the updated Customer Table to a Staging table.\n\n%%spark\n\nimport uuid\n\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)\n\n\n--Use the Polybase Connector to Write to the Staging Table\n%%spark\n\n(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forward_spark_azure_storage_credentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())\n\n\n--Read and Display Changes from Staging Table\n\n%%spark\n\ncustomerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")\n\n%%sql\nselect CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;\n\n\n\n--Use data loading best practices in Azure Synapse Analytics \n-- Simplify ingestion with the Copy Activity\n\n-- Replace YOURACCOUNT with the name of your ADLS Gen2 account.\n\nCREATE EXTERNAL DATA SOURCE ABSS\nWITH\n( TYPE = HADOOP,\n    LOCATION = 'abfss://wwi-02@YOURACCOUNT.dfs.core.windows.net'\n);\n\nCREATE EXTERNAL FILE FORMAT [ParquetFormat]\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n-- To create the external file format and external data table.\n\nCREATE SCHEMA [wwi_external];\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].Sales\n    (\n        [TransactionId] [nvarchar](36)  NOT NULL,\n        [CustomerId] [int]  NOT NULL,\n        [ProductId] [smallint]  NOT NULL,\n        [Quantity] [smallint]  NOT NULL,\n        [Price] [decimal](9,2)  NOT NULL,\n        [TotalAmount] [decimal](9,2)  NOT NULL,\n        [TransactionDate] [int]  NOT NULL,\n        [ProfitAmount] [decimal](9,2)  NOT NULL,\n        [Hour] [tinyint]  NOT NULL,\n        [Minute] [tinyint]  NOT NULL,\n        [StoreId] [smallint]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/sale-small%2FYear%3D2019',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = [ParquetFormat]  \n    )  \nGO\n\n--\n\nINSERT INTO [wwi_staging].[SaleHeap]\nSELECT *\nFROM [wwi_external].[Sales]\n\n-- Simplify ingestion with the COPY activity\n\nTRUNCATE TABLE wwi_staging.SaleHeap;\nGO\n\n-- Replace YOURACCOUNT with the workspace default storage account name.\nCOPY INTO wwi_staging.SaleHeap\nFROM 'https://YOURACCOUNT.dfs.core.windows.net/wwi-02/sale-small%2FYear%3D2019'\nWITH (\n    FILE_TYPE = 'PARQUET',\n    COMPRESSION = 'SNAPPY'\n)\nGO\n\nSELECT COUNT(1) FROM wwi_staging.SaleHeap(nolock)\n\n-- load data from a public storage account. \n-- Here the COPY statement's defaults match the format of the line item csv file.\n\nCOPY INTO dbo.[lineitem] \nFROM 'https://unsecureaccount.blob.core.windows.net/customerdatasets/folder1/lineitem.csv'\n\n-- This example loads files specifying a column list with default values.\n\n--Note when specifying the column list, input field numbers start from 1\nCOPY INTO test_1 (Col_one default 'myStringDefault' 1, Col_two default 1 3)\nFROM 'https://myaccount.blob.core.windows.net/myblobcontainer/folder1/'\nWITH (\n    FILE_TYPE = 'CSV',\n    CREDENTIAL=(IDENTITY= 'Storage Account Key', SECRET='<Your_Account_Key>'),\n\t--CREDENTIAL should look something like this:\n    --CREDENTIAL=(IDENTITY= 'Storage Account Key', SECRET='x6RWv4It5F2msnjelv3H4DA80n0PQW0daPdw43jM0nyetx4c6CpDkdj3986DX5AHFMIf/YN4y6kkCnU8lb+Wx0Pj+6MDw=='),\n    FIELDQUOTE = '\"',\n    FIELDTERMINATOR=',',\n    ROWTERMINATOR='0x0A',\n    ENCODING = 'UTF8',\n    FIRSTROW = 2\n)\n\n-- The following example loads files that use the line feed as a row terminator such as a UNIX output. \n-- This example also uses a SAS key to authenticate to Azure blob storage.\n\nCOPY INTO test_1\nFROM 'https://myaccount.blob.core.windows.net/myblobcontainer/folder1/'\nWITH (\n    FILE_TYPE = 'CSV',\n    CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='<Your_SAS_Token>'),\n\t--CREDENTIAL should look something like this:\n    --CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='?sv=2018-03-28&ss=bfqt&srt=sco&sp=rl&st=2016-10-17T20%3A14%3A55Z&se=2021-10-18T20%3A19%3A00Z&sig=IEoOdmeYnE9%2FKiJDSHFSYsz4AkNa%2F%2BTx61FuQ%2FfKHefqoBE%3D'),\n    FIELDQUOTE = '\"',\n    FIELDTERMINATOR=';',\n    ROWTERMINATOR='0X0A',\n    ENCODING = 'UTF8',\n    DATEFORMAT = 'ymd',\n\tMAXERRORS = 10,\n\tERRORFILE = '/errorsfolder',--path starting from the storage container\n\tIDENTITY_INSERT = 'ON'\n)\n\n-- The data has the following fields: Date, NorthAmerica, SouthAmerica, Europe, Africa, and Asia. \n-- They must process this data and store it in Synapse Analytics.\n\nCREATE TABLE [wwi_staging].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nGO\n\n-- Replace <PrimaryStorage> with the workspace default storage account name.\nCOPY INTO wwi_staging.DailySalesCounts\nFROM 'https://YOURACCOUNT.dfs.core.windows.net/wwi-02/campaign-analytics/dailycounts.txt'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIELDTERMINATOR='.',\n    ROWTERMINATOR=','\n)\nGO\n\nSELECT * FROM [wwi_staging].DailySalesCounts\nORDER BY [Date] DESC\n\n-- Attempt to load using PolyBase\n-- The row delimiter in delimited-text files must be supported by Hadoop's LineRecordReader. \n-- That is, it must be either \\r, \\n, or \\r\\n. These delimiters are not user-configurable.\n\nThis is an example of where COPY's flexibility gives it an advantage over PolyBase.\n\nCREATE EXTERNAL FILE FORMAT csv_dailysales\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = '.',\n        DATE_FORMAT = '',\n        USE_TYPE_DEFAULT = False\n    )\n);\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/campaign-analytics/dailycounts.txt',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = csv_dailysales\n    )  \nGO\nINSERT INTO [wwi_staging].[DailySalesCounts]\nSELECT *\nFROM [wwi_external].[DailySalesCounts]\n\n\n\n--Set-up dedicated data load accounts within Azure Synapse Analytics to\n--optimize load performance and maintain concurrency as required \n--by managing the available resource slots available within the dedicated SQL Pool.\n\n-- Connect to master\nCREATE LOGIN loader WITH PASSWORD = 'a123STRONGpassword!';\n\n-- Connect to the SQL pool\nCREATE USER loader FOR LOGIN loader;\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO loader;\nGRANT INSERT ON <yourtablename> TO loader;\nGRANT SELECT ON <yourtablename> TO loader;\nGRANT CREATE TABLE TO loader;\nGRANT ALTER ON SCHEMA::dbo TO loader;\n\nCREATE WORKLOAD GROUP DataLoads\nWITH ( \n    MIN_PERCENTAGE_RESOURCE = 100\n    ,CAP_PERCENTAGE_RESOURCE = 100\n    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 100\n    );\n\nCREATE WORKLOAD CLASSIFIER [wgcELTLogin]\nWITH (\n        WORKLOAD_GROUP = 'DataLoads'\n    ,MEMBERNAME = 'loader'\n);\n\n--Exercise - implement workload management\n--Create a workload classifier to add importance to certain queries\n--Lab 08 - Execute Data Analyst and CEO Queries\n----First, let's confirm that there are no queries currently being run by users logged in workload01 or workload02\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, \nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') \n--and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,s.login_name\n\n--To see what happened to all the queries we just triggered as they flood the system.\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status\n\n--To give our asa.sql.workload01 user queries priority by implementing the Workload Importance feature.\n\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')\nBEGIN\n    DROP WORKLOAD CLASSIFIER CEO;\nEND\nCREATE WORKLOAD CLASSIFIER CEO\n  WITH (WORKLOAD_GROUP = 'largerc'\n  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);\n\n--\n\nIF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')\nBEGIN\n    DROP WORKLOAD CLASSIFIER CEO;\nEND\nCREATE WORKLOAD CLASSIFIER CEO\n  WITH (WORKLOAD_GROUP = 'largerc'\n  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s \nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())\nORDER BY submit_time ,status desc\n\n--Reserve resources for specific workloads through workload isolation\n\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_groups where name = 'CEODemo')\nBEGIN\n    Create WORKLOAD GROUP CEODemo WITH  \n    ( MIN_PERCENTAGE_RESOURCE = 50        -- integer value\n    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25 --  \n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\nEND\n\n--\n\nIF NOT EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where  name = 'CEODreamDemo')\nBEGIN\n    Create Workload Classifier CEODreamDemo with\n    ( Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n--IF  EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where group_name = 'CEODemo')\nBEGIN\n    Drop Workload Classifier CEODreamDemo\n    DROP WORKLOAD GROUP CEODemo\n    --- Creates a workload group 'CEODemo'.\n        Create  WORKLOAD GROUP CEODemo WITH  \n    (MIN_PERCENTAGE_RESOURCE = 26 -- integer value\n        ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 3.25 -- factor of 26 (guaranteed more than 8 concurrencies)\n    ,CAP_PERCENTAGE_RESOURCE = 100\n    )\n    --- Creates a workload Classifier 'CEODreamDemo'.\n    Create Workload Classifier CEODreamDemo with\n    (Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);\nEND\n\n--\n\nSELECT s.login_name, r.[Status], r.Importance, submit_time,\nstart_time ,s.session_id FROM sys.dm_pdw_exec_sessions s\nJOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id\nWHERE s.login_name IN ('asa.sql.workload02') and Importance\nis  not NULL AND r.[status] in ('Running','Suspended')\nORDER BY submit_time, status\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "CustomerDatabase",
				"poolName": "Built-in"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}