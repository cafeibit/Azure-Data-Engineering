{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Started\r\n",
        "\r\n",
        "You will notice that throughout this course, there is a lot of context switching between PySpark/Scala and SQL.\r\n",
        "\r\n",
        "This is because:\r\n",
        "* `read` and `write` operations are performed on DataFrames using PySpark or Scala\r\n",
        "* table creates and queries are performed directly off Delta Lake tables using SQL"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Key Concepts: Delta Lake Architecture</h2>\r\n",
        "\r\n",
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We'll touch on this further in future notebooks.\r\n",
        "\r\n",
        "Throughout our Delta Lake discussions, we'll often refer to the concept of Bronze/Silver/Gold tables. These levels refer to the state of data refinement as data flows through a processing pipeline.\r\n",
        "\r\n",
        "**These levels are conceptual guidelines, and implemented architectures may have any number of layers with various levels of enrichment.** Below are some general ideas about the state of data in each level.\r\n",
        "\r\n",
        "* **Bronze** tables\r\n",
        "  * Raw data (or very little processing)\r\n",
        "  * Data will be stored in the Delta format (can encode raw bytes as a column)\r\n",
        "* **Silver** tables\r\n",
        "  * Data that is directly queryable and ready for insights\r\n",
        "  * Bad records have been handled, types have been enforced\r\n",
        "* **Gold** tables\r\n",
        "  * Highly refined views of the data\r\n",
        "  * Aggregate tables for BI\r\n",
        "  * Feature tables for data scientists\r\n",
        "\r\n",
        "For different workflows, things like schema enforcement and deduplication may happen in different places."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Lake Batch Operations - Create\r\n",
        "\r\n",
        "Creating Delta Lakes is as easy as changing the file type while performing a write. \r\n",
        "\r\n",
        "In this section, we'll read from a CSV and write to Delta."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://files.training.databricks.com/images/adbcore/AAFxQkg_SzRC06GvVeatDBnNbDL7wUUgCg4B.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data into a DataFrame. We supply the schema.\r\n",
        "\r\n",
        "Use overwrite mode so that there will not be an issue in rewriting the data in case you end up running the cell again.\r\n",
        "\r\n",
        "Partition on `Country` because there are only a few unique countries and because we will use `Country` as a predicate in a `WHERE` clause.\r\n",
        "\r\n",
        "More information on the how and why of partitioning is contained in the links at the bottom of this notebook.\r\n",
        "\r\n",
        "Then write the data to Delta Lake."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\r\n",
        "\r\n",
        "inputSchema = StructType([\r\n",
        "  StructField(\"InvoiceNo\", IntegerType(), True),\r\n",
        "  StructField(\"StockCode\", StringType(), True),\r\n",
        "  StructField(\"Description\", StringType(), True),\r\n",
        "  StructField(\"Quantity\", IntegerType(), True),\r\n",
        "  StructField(\"InvoiceDate\", StringType(), True),\r\n",
        "  StructField(\"UnitPrice\", DoubleType(), True),\r\n",
        "  StructField(\"CustomerID\", IntegerType(), True),\r\n",
        "  StructField(\"Country\", StringType(), True)\r\n",
        "])\r\n",
        "\r\n",
        "rawDataDF = (spark.read\r\n",
        "  .option(\"header\", \"true\")\r\n",
        "  .schema(inputSchema)\r\n",
        "  .csv(inputPath)\r\n",
        ")\r\n",
        "\r\n",
        "# write to Delta Lake\r\n",
        "rawDataDF.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Country\").save(DataPath)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> While we show creating a table in the next section, Spark SQL queries can run directly on a directory of data, for delta use the following syntax: \r\n",
        "```\r\n",
        "SELECT * FROM delta.`/path/to/delta_directory`\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(spark.sql(\"SELECT * FROM delta.`{}` LIMIT 5\".format(DataPath)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATE A Table Using Delta Lake\r\n",
        "\r\n",
        "Create a table called `customer_data_delta` using `DELTA` out of the above data.\r\n",
        "\r\n",
        "The notation is:\r\n",
        "> `CREATE TABLE <table-name>` <br>\r\n",
        "  `USING DELTA` <br>\r\n",
        "  `LOCATION <path-do-data> ` <br>\r\n",
        "  \r\n",
        "Tables created with a specified `LOCATION` are considered unmanaged by the metastore. Unlike a managed table, where no path is specified, an unmanaged table’s files are not deleted when you `DROP` the table. However, changes to either the registered table or the files will be reflected in both locations.\r\n",
        "\r\n",
        "<img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Managed tables require that the data for your table be stored in DBFS. Unmanaged tables only store metadata in DBFS. \r\n",
        "\r\n",
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Since Delta Lake stores schema (and partition) info in the `_delta_log` directory, we do not have to specify partition columns!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\r\n",
        "  DROP TABLE IF EXISTS customer_data_delta\r\n",
        "\"\"\")\r\n",
        "spark.sql(\"\"\"\r\n",
        "  CREATE TABLE customer_data_delta\r\n",
        "  USING DELTA\r\n",
        "  LOCATION '{}'\r\n",
        "\"\"\".format(DataPath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a simple `count` query to verify the number of records.\r\n",
        "\r\n",
        "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Notice how the count is right off the bat; no need to worry about table repairs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "SELECT count(*) FROM customer_data_delta"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata\r\n",
        "\r\n",
        "Since we already have data backing `customer_data_delta` in place,\r\n",
        "the table in the Hive metastore automatically inherits the schema, partitioning,\r\n",
        "and table properties of the existing data.\r\n",
        "\r\n",
        "Note that we only store table name, path, database info in the Hive metastore,\r\n",
        "the actual schema is stored in the `_delta_log` directory as shown below."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(dbutils.fs.ls(DataPath + \"/_delta_log\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata is displayed through `DESCRIBE DETAIL <tableName>`.\r\n",
        "\r\n",
        "As long as we have some data in place already for a Delta Lake table, we can infer schema."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "DESCRIBE DETAIL customer_data_delta"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways\r\n",
        "\r\n",
        "Saving to Delta Lake is as easy as saving to Parquet, but creates an additional log file.\r\n",
        "\r\n",
        "Using Delta Lake to create tables is straightforward and you do not need to specify schemas."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Lake Batch Operations - Append\r\n",
        "\r\n",
        "In this section, we'll load a small amount of new data and show how easy it is to append this to our existing Delta table.\r\n",
        "\r\n",
        "We'll start start by setting up our relevant path and loading new consumer product data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miniDataInputPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-mini.csv\"\r\n",
        "\r\n",
        "newDataDF = (spark\r\n",
        "  .read\r\n",
        "  .option(\"header\", \"true\")\r\n",
        "  .schema(inputSchema)\r\n",
        "  .csv(miniDataInputPath)\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do a simple count of number of new items to be added to production data.\r\n",
        "\r\n",
        "### APPEND Using Delta Lake\r\n",
        "\r\n",
        "Adding to our existing Delta Lake is as easy as modifying our write statement and specifying the `append` mode. \r\n",
        "\r\n",
        "Here we save to our previously created Delta Lake at `delta/customer-data/`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(newDataDF\r\n",
        "  .write\r\n",
        "  .format(\"delta\")\r\n",
        "  .partitionBy(\"Country\")\r\n",
        "  .mode(\"append\")\r\n",
        "  .save(DataPath)\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a simple `count` query to verify the number of records and notice it is correct.\r\n",
        "\r\n",
        "Should be `65535`.\r\n",
        "\r\n",
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The changes to our files have been immediately reflected in the table that we've registered."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways\r\n",
        "With Delta Lake, you can easily append new data without schema-on-read issues.\r\n",
        "\r\n",
        "Changes to Delta Lake files will immediately be reflected in registered Delta tables.\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Lake Batch Operations - Upsert\r\n",
        "\r\n",
        "To UPSERT means to \"UPdate\" and \"inSERT\". In other words, UPSERT is literally TWO operations. It is not supported in traditional data lakes, as running an UPDATE could invalidate data that is accessed by the subsequent INSERT operation.\r\n",
        "\r\n",
        "Using Delta Lake, however, we can do UPSERTS. Delta Lake combines these operations to guarantee atomicity to\r\n",
        "- INSERT a row \r\n",
        "- if the row already exists, UPDATE the row.\r\n",
        "\r\n",
        "### Scenario\r\n",
        "You have a small amount of batch data to write to your Delta table. This is currently staged in a JSON in a mounted blob store."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upsertDF = spark.read.format(\"json\").load(\"/mnt/training/enb/commonfiles/upsert-data.json\")\r\n",
        "display(upsertDF)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll register this as a temporary view so that this table doesn't persist in DBFS (but we can still use SQL to query it)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upsertDF.createOrReplaceTempView(\"upsert_data\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Included in this data are:\r\n",
        "- Some new orders for customer 20993\r\n",
        "- An update to a previous order correcting the country for customer 20993 to Iceland\r\n",
        "- Corrections to some records for StockCode 22837 where the Description was incorrect\r\n",
        "\r\n",
        "We can use UPSERT to simultaneously INSERT our new data and UPDATE our previous records."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "MERGE INTO customer_data_delta\r\n",
        "USING upsert_data\r\n",
        "ON customer_data_delta.InvoiceNo = upsert_data.InvoiceNo\r\n",
        "  AND customer_data_delta.StockCode = upsert_data.StockCode\r\n",
        "WHEN MATCHED THEN\r\n",
        "  UPDATE SET *\r\n",
        "WHEN NOT MATCHED\r\n",
        "  THEN INSERT *"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how this data is seamlessly incorporated into `customer_data_delta`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "SELECT * FROM customer_data_delta WHERE CustomerID=20993"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "SELECT DISTINCT(Description) \r\n",
        "FROM customer_data_delta \r\n",
        "WHERE StockCode = 22837"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\r\n",
        "In this Lesson, we:\r\n",
        "- Saved files using Delta Lake\r\n",
        "- Used Delta Lake to UPSERT data into existing Delta Lake tables"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Topics & Resources\r\n",
        "\r\n",
        "* <a href=\"https://docs.databricks.com/delta/delta-batch.html#\" target=\"_blank\">Table Batch Read and Writes</a>\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Lake Basics Lab\r\n",
        "\r\n",
        "Delta Lake allows you to read, write and query data in data lakes in an efficient manner.\r\n",
        "\r\n",
        "## In this lesson you:\r\n",
        "* Create a new Delta Lake from aggregate data of an existing Delta Lake\r\n",
        "* UPSERT records into a Delta lake\r\n",
        "* Append new data to an existing Delta Lake\r\n",
        "\r\n",
        "## Audience\r\n",
        "* Primary Audience: Data Engineers\r\n",
        "* Secondary Audience: Data Analysts and Data Scientists\r\n",
        "\r\n",
        "## Prerequisites\r\n",
        "* Web browser: current versions of Google Chrome, Firefox, Safari, Microsoft Edge and\r\n",
        "Internet Explorer 11 on Windows 7, 8, or 10 (see <a href=\"https://docs.databricks.com/user-guide/supported-browsers.html#supported-browsers#\" target=\"_blank\">Supported Web Browsers</a>)\r\n",
        "* Databricks Runtime 4.2 or greater\r\n",
        "\r\n",
        "## Datasets Used\r\n",
        "We will use online retail datasets from `/mnt/training/online_retail`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we'll be calculating some aggregates in this notebook, we'll change our partitions after shuffle from the default `200` to `8` (which is a good number for the 8 node cluster we're currently working on)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\r\n",
        "\r\n",
        "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"8\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a table? \r\n",
        "Before we continue, we need to address a semantic concern addressed by the [Databricks docs](https://docs.databricks.com/user-guide/tables.html#view-databases-and-tables):\r\n",
        "\r\n",
        "> A Databricks table is a collection of structured data. Tables are equivalent to Apache Spark DataFrames.\r\n",
        "\r\n",
        "Generally, the distinction between tables and DataFrames in Spark can be summarized by discussing scope and persistence:\r\n",
        "- Tables are defined at the **workspace** level and **persist** between notebooks.\r\n",
        "- DataFrames are defined at the **notebook** level and are **ephemeral**.\r\n",
        "\r\n",
        "When we discuss **Delta tables**, we are always talking about collections of structured data that persist between notebooks. Importantly, we do not need to register a directory of files to Spark SQL in order to refer to them as a table. The directory of files itself _is_ the table; registering it with a useful name to Spark SQL just gives us easy accessing to querying these underlying data.\r\n",
        "\r\n",
        "A **Delta Lake** can be thought of as a collection of one or many Delta tables. Generally, an entire elastic storage container will be dedicated to a single Delta Lake, and data will be enriched and cleaned as it is promoted through pre-defined logic.\r\n",
        "\r\n",
        "<img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> To make Delta tables easily accessible, register them using Spark SQL. Use table ACLs to control access in workspaces shared by many diverse parties within an organization."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a new Delta table\r\n",
        "\r\n",
        "You business intelligence team wants to create a dashboard to track the total number of orders made by customers globally. Many of your customers are international retailers, and have the same customer ID.\r\n",
        "\r\n",
        "Because you batch process your data each day, you've decided to create a workflow that will update their numbers when you run your reports each night.\r\n",
        "\r\n",
        "In this notebook, we'll start by transforming existing data stored in Delta to create a new Delta table for your BI team's dashboard. Then, we'll create processes to append new data to our full records as well as updating the Delta table for the BI team."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that because we registered a global table associated with this Delta table, you should already be able to query this data with SQL. To see all your currently registered tables, click the `Data` icon on the left navigation bar.\r\n",
        "\r\n",
        "<img src=https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/adbcore/data-button.png width=100px>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we stored our data in Delta, our schema and partions are preserved. All we'll need to do is specify the format and the path."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltaDF = (spark.read\r\n",
        "  .format(\"delta\")\r\n",
        "  .load(DeltaPath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate our BI table\r\n",
        "We'll start out by just looking at our aggregate counts. Here, we group by both `\"CustomerID\"` and `\"Country\"`, as it is the combination of these two fields that is of interest to our BI team."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customerCounts = (deltaDF.groupBy(\"CustomerID\", \"Country\")\r\n",
        "  .count()\r\n",
        "  .withColumnRenamed(\"count\", \"total_orders\"))\r\n",
        "\r\n",
        "display(customerCounts)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clicking on the names of the various columns will allow us to quickly sort on different fields. You may notice that we have a large number of entries that are `null` for both `\"CustomerID\"` and `\"Country\"`. While in production, we would like to explore _why_ we are seeing these missing values, for now we'll just leave them as is and save out this DataFrame as a new Delta table.\r\n",
        "\r\n",
        "Here, the path is provided for you."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CustomerCountsPath = userhome + \"/delta/customer_counts/\"\r\n",
        "\r\n",
        "dbutils.fs.rm(CustomerCountsPath, True) #deletes Delta table if previously created"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'll write out our Delta table to the path provided above. Make sure the following settings are provided:\r\n",
        "- `overwrite` (so that this code will work if you run it again)\r\n",
        "- format as `delta`\r\n",
        "- partition by `\"Country\"`\r\n",
        "- save to `CustomerCountsPath`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(customerCounts.write\r\n",
        "  .mode(\"overwrite\")\r\n",
        "  .format(\"delta\")\r\n",
        "  .partitionBy(\"Country\")\r\n",
        "  .save(CustomerCountsPath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also register this Delta table as a Spark SQL table."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\r\n",
        "  DROP TABLE IF EXISTS customer_counts\r\n",
        "\"\"\")\r\n",
        "\r\n",
        "spark.sql(\"\"\"\r\n",
        "  CREATE TABLE customer_counts\r\n",
        "  USING DELTA\r\n",
        "  LOCATION '{}'\r\n",
        "\"\"\".format(CustomerCountsPath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our BI team can quickly query those data points they've expressed interest in."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "\r\n",
        "SELECT *\r\n",
        "FROM customer_counts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at your existing Delta table, you know that a large number of recent orders haven't been loaded in yet."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  READ updated CSV data\r\n",
        "\r\n",
        "Read the data into a DataFrame. We'll use the same schema that we used when creating our Delta table, which is supplied for you."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\r\n",
        "\r\n",
        "inputSchema = StructType([\r\n",
        "  StructField(\"InvoiceNo\", IntegerType(), True),\r\n",
        "  StructField(\"StockCode\", StringType(), True),\r\n",
        "  StructField(\"Description\", StringType(), True),\r\n",
        "  StructField(\"Quantity\", IntegerType(), True),\r\n",
        "  StructField(\"InvoiceDate\", StringType(), True),\r\n",
        "  StructField(\"UnitPrice\", DoubleType(), True),\r\n",
        "  StructField(\"CustomerID\", IntegerType(), True),\r\n",
        "  StructField(\"Country\", StringType(), True)\r\n",
        "])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read data in newDataPath. Re-use inputSchema as defined above. We'll name our DataFrame newDataDF."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newDataPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-small.csv\"\r\n",
        "newDataDF = (spark\r\n",
        " .read\r\n",
        " .option(\"header\", \"true\")\r\n",
        " .schema(inputSchema)\r\n",
        " .csv(newDataPath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the same aggregate count as above, on \"Country\" and \"CustomerID\"."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newCustomerCounts = (newDataDF.groupBy(\"CustomerID\", \"Country\").count().withColumnRenamed(\"count\", \"total_orders\"))\r\n",
        "\r\n",
        "display(newCustomerCounts)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UPSERT new customer counts\r\n",
        "\r\n",
        "Now that we've successfully loaded and aggregated our new data, we can upsert it in our existing Delta Lake.\r\n",
        "\r\n",
        "First, we'll register it as a temp view."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newCustomerCounts.createOrReplaceTempView(\"new_customer_counts\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can merge these new counts into our existing data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "\r\n",
        "MERGE INTO customer_counts\r\n",
        "USING new_customer_counts\r\n",
        "ON customer_counts.Country = new_customer_counts.Country\r\n",
        "AND customer_counts.CustomerID = new_customer_counts.CustomerID\r\n",
        "WHEN MATCHED THEN\r\n",
        "  UPDATE SET total_orders = customer_counts.total_orders + new_customer_counts.total_orders\r\n",
        "WHEN NOT MATCHED THEN\r\n",
        "  INSERT *"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write a simple SQL query to confirm that this has worked."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "\r\n",
        "SELECT SUM(total_orders) FROM customer_counts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update full records using append\r\n",
        "\r\n",
        "We want to retain the full records being generated from our batch report in our existing non-aggregated Delta table.\r\n",
        "\r\n",
        "In this case, we're assuming that the records we process at the end of each day are correct, and that batch processing will result in correct, stable records. We can safely write our table to the same file path using the append mode to insert these records.\r\n",
        "\r\n",
        "**Note**: If our reports included changes to line items from previous days, we would want to write an UPSERT which would allow us to simultaneously update our changed records and insert new data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(newDataDF.write\r\n",
        "  .format(\"delta\")\r\n",
        "  .mode(\"append\")\r\n",
        "  .save(DeltaPath))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Querying our table again shows that we've immediately updated."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "SELECT COUNT(*) FROM customer_data_delta"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Topics & Resources\r\n",
        "\r\n",
        "* <a href=\"https://docs.databricks.com/delta/delta-batch.html#\" target=\"_blank\">Table Batch Read and Writes</a>\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://files.training.databricks.com/images/DeltaLake-logo.png\" width=80px> Managed Delta Lake\r\n",
        "\r\n",
        "Delta Lake&reg; managed and queried via the Databricks platform includes additional features and optimizations.\r\n",
        "\r\n",
        "These include:\r\n",
        "\r\n",
        "- **Optimize**\r\n",
        "\r\n",
        "- **Data skipping**\r\n",
        "\r\n",
        "- **Z-Order**\r\n",
        "\r\n",
        "- **Caching**\r\n",
        "\r\n",
        "<img src=\"https://www.evernote.com/l/AAGv1SuWeRNJM4TI4bIOyGNPm0CTHa17PLwB/image.png\" width=900px>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\r\n",
        "    DROP TABLE IF EXISTS iot_data\r\n",
        "  \"\"\")\r\n",
        "spark.sql(\"\"\"\r\n",
        "    CREATE TABLE iot_data\r\n",
        "    USING DELTA\r\n",
        "    LOCATION '{}/delta/iot-events/'\r\n",
        "  \"\"\".format(userhome))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up relevant paths."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iotPath = userhome + \"/delta/iot-events/\""
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMALL FILE PROBLEM\r\n",
        "\r\n",
        "Historical and new data is often written in very small files and directories.\r\n",
        "\r\n",
        "This data may be spread across a data center or even across the world (that is, not co-located).\r\n",
        "\r\n",
        "The result is that a query on this data may be very slow due to\r\n",
        "* network latency\r\n",
        "* volume of file metatadata\r\n",
        "\r\n",
        "The solution is to compact many small files into one larger file.\r\n",
        "Delta Lake has a mechanism for compacting small files."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTIMIZE\r\n",
        "Delta Lake supports the `OPTIMIZE` operation, which performs file compaction.\r\n",
        "\r\n",
        "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Small files are compacted together into new larger files up to 1GB.\r\n",
        "Thus, at this point the number of files increases!\r\n",
        "\r\n",
        "The 1GB size was determined by the Databricks optimization team as a trade-off between query speed and run-time performance when running Optimize.\r\n",
        "\r\n",
        "`OPTIMIZE` is not run automatically because you must collect many small files first.\r\n",
        "\r\n",
        "* Run `OPTIMIZE` more often if you want better end-user query performance\r\n",
        "* Since `OPTIMIZE` is a time consuming step, run it less often if you want to optimize cost of compute hours\r\n",
        "* To start with, run `OPTIMIZE` on a daily basis (preferably at night when spot prices are low), and determine the right frequency for your particular business case\r\n",
        "* In the end, the frequency at which you run `OPTIMIZE` is a business decision\r\n",
        "\r\n",
        "The easiest way to see what `OPTIMIZE` does is to perform a simple `count(*)` query before and after and compare the timing!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the `iotPath + \"/date=2018-06-01/\" ` directory.\r\n",
        "\r\n",
        "Notice, in particular files like `../delta/iot-events/date=2018-07-26/part-xxxx.snappy.parquet`. There are hundreds of small files!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(dbutils.fs.ls(iotPath + \"/date=2016-07-26\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CAUTION: Run this query. Notice it is very slow, due to the number of small files."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "SELECT * FROM iot_data where deviceId=92"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Skipping and ZORDER\r\n",
        "\r\n",
        "Delta Lake uses two mechanisms to speed up queries.\r\n",
        "\r\n",
        "<b>Data Skipping</b> is a performance optimization that aims at speeding up queries that contain filters (WHERE clauses).\r\n",
        "\r\n",
        "For example, we have a data set that is partitioned by `date`.\r\n",
        "\r\n",
        "A query using `WHERE date > 2016-07-26` would not access data that resides in partitions that correspond to dates prior to `2016-07-26`.\r\n",
        "\r\n",
        "<b>ZOrdering</b> is a technique to colocate related information in the same set of files.\r\n",
        "\r\n",
        "ZOrdering maps multidimensional data to one dimension while preserving locality of the data points.\r\n",
        "\r\n",
        "Given a column that you want to perform ZORDER on, say `OrderColumn`, Delta\r\n",
        "* takes existing parquet files within a partition\r\n",
        "* maps the rows within the parquet files according to `OrderColumn` using the algorithm described <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">here</a>\r\n",
        "* (in the case of only one column, the mapping above becomes a linear sort)\r\n",
        "* rewrites the sorted data into new parquet files\r\n",
        "\r\n",
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You cannot use the partition column also as a ZORDER column."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ZORDER Technical Overview\r\n",
        "\r\n",
        "A brief example of how this algorithm works (refer to [this blog](https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html) for more details):\r\n",
        "\r\n",
        "![](https://files.training.databricks.com/images/adbcore/zorder.png)\r\n",
        "\r\n",
        "Legend:\r\n",
        "- Gray dot = data point e.g., chessboard square coordinates\r\n",
        "- Gray box = data file; in this example, we aim for files of 4 points each\r\n",
        "- Yellow box = data file that’s read for the given query\r\n",
        "- Green dot = data point that passes the query’s filter and answers the query\r\n",
        "- Red dot = data point that’s read, but doesn’t satisfy the filter; “false positive”"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ZORDER example\r\n",
        "In the image below, table `Students` has 4 columns:\r\n",
        "* `gender` with 2 distinct values\r\n",
        "* `Pass-Fail` with 2 distinct values\r\n",
        "* `Class` with 4 distinct values\r\n",
        "* `Student` with many distinct values\r\n",
        "\r\n",
        "Suppose you wish to perform the following query:\r\n",
        "\r\n",
        "```SELECT Name FROM Students WHERE gender = 'M' AND Pass_Fail = 'P' AND Class = 'Junior'```\r\n",
        "\r\n",
        "```ORDER BY Gender, Pass_Fail```\r\n",
        "\r\n",
        "The most effective way of performing that search is to order the data starting with the largest set, which is `Gender` in this case.\r\n",
        "\r\n",
        "If you're searching for `gender = 'M'`, then you don't even have to look at students with `gender = 'F'`.\r\n",
        "\r\n",
        "Note that this technique only works if all `gender = 'M'` values are co-located.\r\n",
        "\r\n",
        "\r\n",
        "<div><img src=\"https://files.training.databricks.com/images/eLearning/Delta/zorder.png\" style=\"height: 300px\"/></div><br/>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ZORDER usage\r\n",
        "\r\n",
        "With Delta Lake the notation is:\r\n",
        "\r\n",
        "> `OPTIMIZE Students`<br>\r\n",
        "`ZORDER BY Gender, Pass_Fail`\r\n",
        "\r\n",
        "This will ensure all the data backing `Gender = 'M' ` is colocated, then data associated with `Pass_Fail = 'P' ` is colocated.\r\n",
        "\r\n",
        "See References below for more details on the algorithms behind ZORDER.\r\n",
        "\r\n",
        "Using ZORDER, you can order by multiple columns as a comma separated list; however, the effectiveness of locality drops.\r\n",
        "\r\n",
        "In streaming, where incoming events are inherently ordered (more or less) by event time, use `ZORDER` to sort by a different column, say 'userID'."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "OPTIMIZE iot_data\r\n",
        "ZORDER by (deviceId)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "SELECT * FROM iot_data WHERE deviceId=92"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VACUUM\r\n",
        "\r\n",
        "To save on storage costs you should occasionally clean up invalid files using the `VACUUM` command.\r\n",
        "\r\n",
        "Invalid files are small files compacted into a larger file with the `OPTIMIZE` command.\r\n",
        "\r\n",
        "The  syntax of the `VACUUM` command is\r\n",
        ">`VACUUM name-of-table RETAIN number-of HOURS;`\r\n",
        "\r\n",
        "The `number-of` parameter is the <b>retention interval</b>, specified in hours.\r\n",
        "\r\n",
        "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Databricks does not recommend you set a retention interval shorter than seven days because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table.\r\n",
        "\r\n",
        "The scenario here is:\r\n",
        "0. User A starts a query off uncompacted files, then\r\n",
        "0. User B invokes a `VACUUM` command, which deletes the uncompacted files\r\n",
        "0. User A's query fails because the underlying files have disappeared\r\n",
        "\r\n",
        "Invalid files can also result from updates/upserts/deletions.\r\n",
        "\r\n",
        "More details are provided here: <a href=\"https://docs.databricks.com/delta/optimizations.html#garbage-collection\" target=\"_blank\"> Garbage Collection</a>."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dbutils.fs.ls(iotPath + \"date=2016-07-26\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> In the example below we set off an immediate `VACUUM` operation with an override of the retention check so that all files are cleaned up immediately.\r\n",
        "\r\n",
        "Do not do this in production!\r\n",
        "\r\n",
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If using Databricks Runtime 5.1, in order to use a retention time of 0 hours, the following flag must be set."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\r\n",
        "\r\n",
        "VACUUM iot_data RETAIN 0 HOURS;"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how the directory looks vastly cleaned up!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dbutils.fs.ls(iotPath + \"date=2016-07-26\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\r\n",
        "Delta Lake offers key features that allow for query optimization and garbage collection, resulting in improved performance."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Topics & Resources\r\n",
        "\r\n",
        "* <a href=\"https://docs.databricks.com/delta/optimizations.html#\" target=\"_blank\">Optimizing Performance and Cost</a>\r\n",
        "* <a href=\"http://parquet.apache.org/documentation/latest/\" target=\"_blank\">Parquet Metadata</a>\r\n",
        "* <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">Z-Order Curve</a>\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}